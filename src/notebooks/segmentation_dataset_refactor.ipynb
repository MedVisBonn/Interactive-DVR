{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "import nrrd\n",
    "import random\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Iterable, Dict, Callable, Tuple, Union\n",
    "from scipy.ndimage import binary_erosion, binary_dilation\n",
    "import random\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "from model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        ground_truth: Tensor, \n",
    "        cfg: dict, \n",
    "        # brush_sizes=torch.arange(2,7)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # globals\n",
    "        self.gt = ground_truth.float() # nd array or float tensor?\n",
    "        # if 'brush' in cfg.keys():\n",
    "        self.brush_sizes = cfg['brush_sizes']\n",
    "        # else:\n",
    "        #     self.brush_sizes = [1]\n",
    "        \n",
    "        if cfg['slice_selection'] == 'mean':\n",
    "            self.slice_selection = 'mean'\n",
    "        elif cfg['slice_selection'] == 'max':\n",
    "            self.slice_selection = 'max'\n",
    "        else:\n",
    "            raise ValueError('Invalid slice selection method. Choose between \"mean\" and \"max\".')\n",
    "\n",
    "        if cfg['voxel_selection'] == 'mean':\n",
    "            self.voxel_selection = 'mean'\n",
    "        elif cfg['voxel_selection'] == 'max':\n",
    "            self.voxel_selection = 'max'\n",
    "        else:\n",
    "            raise ValueError('Invalid voxel selection method. Choose between \"mean\" and \"max\".')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # statistics to track\n",
    "        self.annotated_pixels = None\n",
    "        self.annotated_slices = None\n",
    "        \n",
    "\n",
    "    def _sum_l1_per_slice(\n",
    "        self, \n",
    "        volume: Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\" Find sum over all slices in each direction\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        volume : Tensor \n",
    "            shape L x W x H with L = W = H  \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : Tensor\n",
    "            (N, n) shaped array holding slice sums, where N is\n",
    "            the dimensionality (3) and n the number of slices in each\n",
    "            direction (L, W, H). Expects zero padding to ensure same length\n",
    "            across dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "        assert((volume.shape[0] == volume.shape[1]) and (volume.shape[0] == volume.shape[2]))\n",
    "\n",
    "        # dimensionality and array of possible axis for volume\n",
    "        dims    = len(volume.shape)\n",
    "        indices = torch.arange(dims)\n",
    "        sums    = torch.zeros((dims, volume.shape[0]))\n",
    "\n",
    "        # for each direction, sum values in each slice\n",
    "        for dim in range(dims):\n",
    "            axis       = tuple(indices[indices != dim])\n",
    "            slice_sums = volume.sum(axis=axis)\n",
    "            sums[dim]  = slice_sums\n",
    "        return sums\n",
    "        \n",
    "        \n",
    "    def _order_slices_by_sum(\n",
    "        self, \n",
    "        slice_sums: Tensor\n",
    "    ) -> Union[np.array, np.array]:\n",
    "        \"\"\" Order slices by their overall sum\n",
    "        Note: numpy dependent, because np.unravel_index exists\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        slice_sums : Tensor\n",
    "            shape (N, n) with N=3 and n=L=H=W\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        axis : 1d array\n",
    "            axis of slices in descending order w.r.t.\n",
    "            their slice sum\n",
    "        slices : 1d array\n",
    "            slice index of slices in descending order\n",
    "            w.r.t. their slice sum\n",
    "        \"\"\"\n",
    "\n",
    "        length = slice_sums.shape[0]\n",
    "\n",
    "        # calculate direction (axis) and indices of slices in \n",
    "        # descending order w.r.t. their slice sum\n",
    "        sorted_slice_indices = torch.argsort(slice_sums.flatten(), descending=True)\n",
    "        # Note: Unravel_index is not yet implemented in torch, but a requested feature\n",
    "        #       as of Dec 2020. Maybe add later. \n",
    "        axis, slices = np.unravel_index(sorted_slice_indices, slice_sums.shape)\n",
    "        \n",
    "        return axis, slices\n",
    "\n",
    "\n",
    "    def _slice_samples_per_class(\n",
    "        self, \n",
    "        slc: Tensor, \n",
    "        inverse_frequencies: Tensor,\n",
    "        n: int\n",
    "    ) -> Tensor:\n",
    "        \"\"\" samples seeds for each class in a slice from the\n",
    "            error map, weighted by inverse class frequencies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        slc : Tensor\n",
    "            slice from error map, shape n_classes x W x H\n",
    "\n",
    "        inverse_frequencies : Tensor\n",
    "            inverse class frequencies from ground truth, shape n_classes x 1 x 1 x 1\n",
    "\n",
    "        n : int\n",
    "            number of seeds\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        n_samples : Tensor\n",
    "            shape n_classes\n",
    "\n",
    "        \"\"\"\n",
    "        # omit sign for number of misclassifications\n",
    "        slc_abs = torch.abs(slc)\n",
    "\n",
    "        # calculate proportions by dividing total number of \n",
    "        # misclassifications by class frequency for each class\n",
    "        # and then normalize to stochastic vector\n",
    "        total                  = slc_abs.sum(dim=(1,2))\n",
    "        proportions            = total * inverse_frequencies.flatten()\n",
    "        proportions_normalized = proportions / proportions.sum()\n",
    "\n",
    "        # quantize sample proportions to get preliminary number\n",
    "        # of samples\n",
    "        n_samples = (proportions_normalized * n).type(torch.int)\n",
    "\n",
    "        # catch cases where int conversion results in a number of\n",
    "        # samples that is different from n and correct them.\n",
    "        # Current strategy: minimal impact by removing and adding\n",
    "        # to dominant class\n",
    "        while ( n_samples.sum() != n ):\n",
    "            # in case of undershoot, add samples to class\n",
    "            # with highest number of overall samples\n",
    "            if n_samples.sum() < n:\n",
    "                n_samples[np.argmax(proportions_normalized)] += 1\n",
    "\n",
    "            # in case of overshoot, remove samples from class\n",
    "            # with highest number of overall samples        \n",
    "            else:\n",
    "                n_samples[np.argmax(proportions_normalized)] -= 1\n",
    "\n",
    "        print(n_samples)\n",
    "        return n_samples\n",
    "\n",
    "\n",
    "    def _sample_candidate_voxels(\n",
    "        self, slc: Tensor, \n",
    "        ground_truth_slice: Tensor, \n",
    "        n_class_samples: Tensor, \n",
    "        seed=None\n",
    "    ) -> Tensor:\n",
    "        \"\"\" individually sample voxels for each class with samples sizes\n",
    "            potentially varying among them.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        slc : Tensor\n",
    "            slice from error map, shape n_classes x W x H\n",
    "            \n",
    "        ground_truth_slice : Tensor\n",
    "            ground truth slice, shape n_classes x W x H\n",
    "            \n",
    "        n_class_samples : Tensor\n",
    "            number of samples for each class, shape n_classes\n",
    "            \n",
    "        seed : int\n",
    "            If not None (default), set specified seed\n",
    "            before sampling.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples : Tensor\n",
    "            mask with samples for specified slice and all classes,\n",
    "            shape n_classes x W x H\n",
    "        \"\"\"\n",
    "\n",
    "        # seed if specified\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        # init sampler and output tensor\n",
    "        sampler = torch.utils.data.WeightedRandomSampler\n",
    "        samples = torch.zeros_like(slc)\n",
    "\n",
    "        #weights = torch.any(torch.abs(slc).type(torch.uint8), axis=0) * ground_truth_slice # 5 x 145 x 145 , alte Version\n",
    "        weights = torch.abs(slc).max(dim=0).values * ground_truth_slice # 5 x 145 x 145 , neue Version\n",
    "\n",
    "        # print(weights.shape, (weights>0).sum(axis=(1,2)))\n",
    "        weights = weights / (weights>0).sum(axis=(1,2)).reshape(-1,1,1)\n",
    "        # iterate over classes, sampling for each independently\n",
    "        for weight, num_samples, i in zip(weights, n_class_samples, range(slc.shape[0])):\n",
    "            \n",
    "            # generate uniform weights for false negative voxels\n",
    "            # weight = (volume > 0) / (volume > 0).sum()\n",
    "            # upper bound for number of samples to maximum in slice\n",
    "            max_samples = (weight > 0).sum()\n",
    "            num_samples = int(min(max_samples, num_samples))\n",
    "            # catch case where number of samples is zero for a class\n",
    "            if num_samples > 0:\n",
    "                # 1D coordinates for samples from weight matrix\n",
    "                #print(num_samples)\n",
    "                index_list = list(sampler(weight.flatten(), num_samples=num_samples, replacement=False))\n",
    "                # 2D coordinates for samples from weight matrix\n",
    "                index_coords = np.unravel_index(index_list, weight.shape)\n",
    "                # apply mask via coordinates to samples for class i\n",
    "                # print(samples.shape)\n",
    "                for l in range(len(samples)):\n",
    "                    samples[l][index_coords] = ground_truth_slice[l][index_coords]\n",
    "                # print(samples.sum())\n",
    "                # # samples[i][index_coords] = 1   # alte Version, annotiert nur aktuell betrachtete Klasse\n",
    "                # samples.view(n_class_samples.shape[0], -1)[index_list] = ground_truth_slice.view(n_class_samples.shape[0], -1)[index_list]\n",
    "                # # samples[:, index_coords] = ground_truth_slice[:, index_coords] # TODO: get this to run instead of the old version\n",
    "                # print(samples.sum())\n",
    "                #print(index_coords)\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def sample_random_candidate_voxels(\n",
    "        self, \n",
    "        slc: Tensor, \n",
    "        ground_truth_slice: Tensor, \n",
    "        n_samples, \n",
    "        seed=None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        # seed if specified\n",
    "        #if seed is not None:\n",
    "        #    torch.manual_seed(seed)\n",
    "\n",
    "        # init sampler and output tensor\t\n",
    "        sampler = torch.utils.data.WeightedRandomSampler\n",
    "        samples = torch.zeros_like(slc)\n",
    "\n",
    "        weights = torch.any(torch.abs(slc).type(torch.uint8), axis=0) # 145 x 145\n",
    "        weights = weights / (weights>0).sum(axis=(0,1))\n",
    "\n",
    "        num_samples = int(min((weights > 0).sum(), n_samples))\n",
    "\n",
    "        index_list = list(sampler(weights.flatten(), num_samples=num_samples, replacement=False))\n",
    "        index_coords = np.unravel_index(index_list, weights.shape)\n",
    "\n",
    "        samples[:, index_coords] = ground_truth_slice[:, index_coords]\n",
    "\n",
    "        return samples # has to have shape [5, 145, 145]\n",
    "\n",
    "\n",
    "    def _slice_add_neighbors(self, class_samples: Tensor, ground_truth_slice: Tensor) -> Tensor:\n",
    "        \"\"\" creates slice with all sampled interaction candidates and their\n",
    "            neighborhoods for each class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        class_samples : Tensor\n",
    "            sampled seed mask, shape n_classes x W x H \n",
    "    \n",
    "        ground_truth : Tensor\n",
    "            ground truth slice, shape n_classes x W x H \n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interaction_mask : Tensor\n",
    "            mask with added neighbors from brushing,\n",
    "            shape n_classes x W x H\n",
    "        \"\"\"\n",
    "        interaction_mask           = torch.zeros_like(class_samples, dtype=torch.int64)\n",
    "        vectorized_binary_erosion  = np.vectorize(binary_erosion,  signature='(j,i),(k,k)->(j,i)')\n",
    "        vectorized_binary_dilation = np.vectorize(binary_dilation, signature='(j,i),(k,k)->(j,i)')\n",
    "\n",
    "\n",
    "        for size in self.brush_sizes:\n",
    "            brush             = torch.ones((size,size))\n",
    "            brushable_samples = class_samples * torch.tensor(vectorized_binary_erosion(ground_truth_slice, structure=brush))\n",
    "            brushed_samples   = vectorized_binary_dilation(brushable_samples, structure=brush).astype(int)\n",
    "            interaction_mask  = torch.bitwise_or(interaction_mask, torch.tensor(brushed_samples))\n",
    "        \n",
    "        return interaction_mask\n",
    "\n",
    "\n",
    "    def initial_annotation(\n",
    "        self, \n",
    "        n_samples: int, \n",
    "        init: str = 'three_slices', \n",
    "        pos_weight: float = 1, \n",
    "        seed: int = 42\n",
    "    ) -> Tensor:        \n",
    "        \"\"\" creates the initial annotations. For each direction (saggital, coronal,\n",
    "            axial), select the slice with the most foreground labels (3 in total).\n",
    "            (2) For each slice, sample n_samples many seeding points and\n",
    "                save their position in an annotation mask.\n",
    "            (3) Apply the largest quadratic brush (from a given range) to each seed\n",
    "                for which all affected voxels are foreground and add them to\n",
    "                the annotation mask as well.\n",
    "            (4) Mask the ground truth labels with the annotation mask and return.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            number of seed points for each slice. Not needed when \n",
    "            init = paper_init\n",
    "        \n",
    "        init : str\n",
    "            'three_slices': Finds three slices (one in each direction) that have\n",
    "                a high label densety across all classes and then annotates them\n",
    "                partly.\n",
    "            'paper_init': the initial annotation matches the annotation used in \n",
    "                preliminary paper.\n",
    "            'per_class': For each class, finds one slice with high label density\n",
    "                and annotates it partly. The annotations per slice are distributed\n",
    "                according to #TODO\n",
    "        \n",
    "        pos_weight : weight for forground vs background samples per slice. Only used\n",
    "            if init='per_class'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interaction_map : Tensor\n",
    "            shape n_classes x L x W x H\n",
    "\n",
    "        \"\"\"\n",
    "        interaction_map = torch.zeros_like(self.gt).int()\n",
    "        if init == 'paper_init':\n",
    "            # [(\"sagittal\", 72) -> 72, (\"coronal\", 87) -> 72, (\"axial\", 72) -> 72]\n",
    "            \n",
    "            for orientation in range(3):\n",
    "                selection = [slice(None)] * 4\n",
    "                if orientation == 0:\n",
    "                    selection[orientation + 1]  = [73]\n",
    "                else:\n",
    "                    selection[orientation + 1]  = [72]\n",
    "                interaction_map[selection] = self.gt[selection]\n",
    "\n",
    "        elif init == 'three_slices':\n",
    "            n_classes = self.gt.shape[0]\n",
    "            #t = self.gt\n",
    "\n",
    "            inverse_size_weights = self.gt.mean((1,2,3)).sum() / self.gt.mean((1,2,3)).reshape((n_classes,1,1,1))\n",
    "            \n",
    "            \n",
    "            t_norm = torch.norm(self.gt * inverse_size_weights, p=1, dim=0)\n",
    "\n",
    "            # 1.1) calc sum over l1 norms, e.g. for the l1 norms for segmentation predictions\n",
    "            slice_sums = self._sum_l1_per_slice(t_norm)\n",
    "\n",
    "            # 1.2) order slices in descending order by their sum\n",
    "            axis, indices = self._order_slices_by_sum(slice_sums)\n",
    "\n",
    "            # save data location for later sub - sampling\n",
    "            data_location = []\n",
    "\n",
    "            # select one slice for each direction in volume.\n",
    "            for orientation in range(3):\n",
    "                #Use the ordered slice list to find best slice for each direction\n",
    "                selection = [slice(None)] * 4\n",
    "                index     = indices[np.argmax(axis == orientation)]\n",
    "                selection[orientation+1] = index\n",
    "                # select slice from misclassifications and ground truth\n",
    "                t_selection = self.gt[selection]\n",
    "\n",
    "                # samples voxels and add their neighborhood to 2D mask\n",
    "                n_class_samples = self._slice_samples_per_class(t_selection, inverse_size_weights, n_samples // 3) \n",
    "                #print(n_class_samples.sum())\n",
    "                #print(t_selection.shape, n_class_samples)\n",
    "\n",
    "                class_samples = self._sample_candidate_voxels(t_selection, t_selection, n_class_samples=n_class_samples, seed=seed)\n",
    "                brushed_mask  = self._slice_add_neighbors(class_samples, t_selection)\n",
    "                # make interaction map with same shape as model input\n",
    "                interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "                # interaction_map[selection] = ((interaction_map[selection].sum(0) * t_selection) > 0) * 1\n",
    "                data_location.append((orientation, index))\n",
    "        \n",
    "        elif init == 'per_class':\n",
    "            n_classes = self.gt.shape[0]\n",
    "            data_location = []\n",
    "            inverse_size_weights = self.gt.mean((1,2,3)).sum() / self.gt.mean((1,2,3)).reshape((n_classes,1,1,1))\n",
    "            \n",
    "            for c in range(n_classes):\n",
    "                cweight = torch.eye(n_classes)[c].view(n_classes, 1, 1, 1)\n",
    "                t_norm = torch.norm(self.gt * cweight, p=1, dim=0)\n",
    "                slice_sums = self._sum_l1_per_slice(t_norm)\n",
    "                axis, indices = self._order_slices_by_sum(slice_sums)\n",
    "                \n",
    "                selection = [slice(None)] * 4\n",
    "                selection[axis[0] + 1] = indices[0]\n",
    "                \n",
    "                t_selection = self.gt[selection]\n",
    "                \n",
    "                slice_sample_weights = inverse_size_weights * cweight * n_classes / (n_classes+1) * pos_weight + \\\n",
    "                                    inverse_size_weights * (1-cweight) / ( (n_classes+1) * (n_classes-1) )\n",
    "                \n",
    "                n_class_samples = self._slice_samples_per_class(t_selection, slice_sample_weights, n_samples)\n",
    "                #print(n_class_samples, n_samples)\n",
    "                class_samples = self._sample_candidate_voxels(t_selection, t_selection, n_class_samples=n_class_samples, seed=seed)\n",
    "\n",
    "                brushed_mask  = self._slice_add_neighbors(class_samples, t_selection)\n",
    "                interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "                data_location.append((axis[0], indices[0]))\n",
    "\n",
    "        return interaction_map.float()\n",
    "        \n",
    "         \n",
    "    def refinement_annotation(\n",
    "        self,\n",
    "        prediction: Tensor, \n",
    "        annotation_mask: Tensor, \n",
    "        uncertainty_map: Tensor,\n",
    "        n_samples: int, \n",
    "        mode: int = 'single_slice', \n",
    "        pos_weight: float = 1, \n",
    "        seed: int = 42\n",
    "    ) -> Tensor:\n",
    "        \"\"\" Finds the slice with the worst prediction across all three axis and \n",
    "            annotates parts of it. The annotation happens in multiple steps:\n",
    "            (1) mask all voxels that are already annotated with annotation_mask\n",
    "            (2) Sample n_samples many seeding points and save their position in\n",
    "                an annotation mask\n",
    "            (3) Apply the largest quadratic brush (from a given range) to each seed\n",
    "                for which all affected voxels are foreground and add them to\n",
    "                the annotation mask as well.\n",
    "            (4) Mask the ground truth labels with the annotation mask and return\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction : Tensor\n",
    "            predictions of segmentation model with\n",
    "            shape n_classes x L x W x H\n",
    "\n",
    "        annotation_mask : Tensor\n",
    "            current annotation, shape n_classes x L x W x H\n",
    "\n",
    "        n_samples : int\n",
    "            number of samples per slice before brushing\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interaction_map : Tensor\n",
    "            new annotations, shape n_classes x L x W x H\n",
    "\n",
    "        \"\"\"\n",
    "        n_classes = prediction.shape[0]\n",
    "\n",
    "        # calculate inverse class frequencies\n",
    "        inverse_size_weights = self.gt.mean((1,2,3)).sum() / self.gt.mean((1,2,3)).reshape((n_classes,1,1,1))\n",
    "\n",
    "        # calculate mask for available voxels\n",
    "        #available_voxels = 1 - annotation_mask.float() # alte Version\n",
    "        available_voxels = 1 - torch.any(annotation_mask, dim=0, keepdim=True) * 1\n",
    "\n",
    "        # calculate difference between truth and prediction, i.e. misclassified voxels\n",
    "        if uncertainty_map != None:\n",
    "            diff = uncertainty_map * available_voxels\n",
    "        else:\n",
    "            diff = torch.abs(self.gt - prediction.float()) * available_voxels\n",
    "        # print(\"weights:\",inverse_size_weights.flatten())\n",
    "        \n",
    "        \n",
    "        if mode == 'single_slice':\n",
    "            # norm over classes weighted by inverse class frequency - importance weight for sampling\n",
    "            diff_norm = torch.norm(diff  * inverse_size_weights, p=1, dim=0)\n",
    "\n",
    "            # 1.1) calc sum over l1 norms, e.g. for the l1 norms for segmentation predictions\n",
    "            slice_sums = self._sum_l1_per_slice(diff_norm)\n",
    "\n",
    "            # 1.2) order slices in descending order by their sum\n",
    "            axis, indices = self._order_slices_by_sum(slice_sums)\n",
    "\n",
    "            # 2.0) select slice with highest importance weight over all axes\n",
    "            random_selection = np.random.randint(0,6)\n",
    "            ax  = axis[0]\n",
    "            slc = indices[0]\n",
    "            data_location = (ax, slc)\n",
    "            selection = [slice(None)] + [slice(None)] * 3\n",
    "            selection[ax + 1] = slc\n",
    "\n",
    "            # 2.1) calculate number of samples for each class from a raw difference slice\n",
    "            diff_selection  = diff[selection]\n",
    "            t_selection     = self.gt[selection]\n",
    "            n_class_samples = self._slice_samples_per_class(diff_selection, inverse_size_weights, n_samples)\n",
    "            #print(n_class_samples.sum())\n",
    "\n",
    "            # 2.2) for each class, sample from false negatives as often as specified in n_class_samples\n",
    "            class_samples = self._sample_candidate_voxels(diff_selection, t_selection, n_class_samples=n_class_samples, seed=seed)\n",
    "\n",
    "            # 2.3) brush all samples with maximum brush from list of brushes\n",
    "            brushed_mask = self._slice_add_neighbors(class_samples, t_selection)\n",
    "\n",
    "            # 2.4) create interaction map to return\n",
    "            interaction_map = torch.zeros_like(self.gt, dtype=torch.int64)\n",
    "            interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "            # interaction_map[selection] = ((interaction_map[selection].sum(0) * t_selection) > 0) * 1\n",
    "                \n",
    "        elif mode == 'per_class':\n",
    "            data_location = []\n",
    "            interaction_map = torch.zeros_like(self.gt, dtype=torch.int64)\n",
    "            for c in range(n_classes):\n",
    "                cweight = torch.eye(n_classes)[c].view(n_classes, 1, 1, 1)\n",
    "                diff_norm = torch.norm(diff * cweight, p=1, dim=0) # 145, 145, 145, binär\n",
    "\n",
    "                # 1.1) calc sum over l1 norms, e.g. for the l1 norms for segmentation predictions\n",
    "                slice_sums = self._sum_l1_per_slice(diff_norm)  # 3, 145\n",
    "\n",
    "                # 1.2) order slices in descending order by their sum\n",
    "                axis, indices = self._order_slices_by_sum(slice_sums)\n",
    "\n",
    "                # 2.0) select slice with highest importance weight over all axes\n",
    "                random_selection = np.random.randint(0,6)\n",
    "                ax  = axis[0]\n",
    "                slc = indices[0]\n",
    "                \n",
    "                selection = [slice(None)] + [slice(None)] * 3\n",
    "                selection[ax + 1] = slc\n",
    "\n",
    "                # 2.1) calculate number of samples for each class from a raw difference slice\n",
    "                diff_selection = diff[selection]\n",
    "                t_selection    = self.gt[selection]\n",
    "                \n",
    "                slice_sample_weights = inverse_size_weights * cweight * n_classes / (n_classes+1) * pos_weight + \\\n",
    "                    inverse_size_weights * (1-cweight) / ( (n_classes+1) * (n_classes-1) )\n",
    "                \n",
    "                n_class_samples = self._slice_samples_per_class(t_selection, slice_sample_weights, n_samples)\n",
    "                #print(n_class_samples, n_samples)\n",
    "                \n",
    "                # 2.2) for each class, sample from false negatives as often as specified in n_class_samples\n",
    "                class_samples = self._sample_candidate_voxels(diff_selection, t_selection, n_class_samples=n_class_samples, seed=seed)\n",
    "\n",
    "                # 2.3) brush all samples with maximum brush from list of brushes\n",
    "                brushed_mask = self._slice_add_neighbors(class_samples, t_selection)\n",
    "\n",
    "                # 2.4) create interaction map to return\n",
    "                interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "                data_location.append((axis[0], indices[0]))\n",
    "                \n",
    "        return interaction_map.float() # , selection\n",
    "\n",
    "    \n",
    "    def random_refinement_annotation(\n",
    "        self, \n",
    "        prediction: Tensor, \n",
    "        annotation_mask: Tensor,\n",
    "        brain_mask: Tensor, \n",
    "        n_samples: int, \n",
    "        mode: int = 'single_slice', \n",
    "        pos_weight: float = 1, \n",
    "        seed: int = 42\n",
    "    ) -> Tensor:\n",
    "        \"\"\" Finds the slice with the highest uncertainty across all three axis and \n",
    "            annotates parts of it. The annotation happens in multiple steps:\n",
    "            (1) mask all voxels that are already annotated with annotation_mask\n",
    "            (2) Sample n_samples many seeding points and save their position in\n",
    "                an annotation mask\n",
    "            (3) Apply the largest quadratic brush (from a given range) to each seed\n",
    "                for which all affected voxels are foreground and add them to\n",
    "                the annotation mask as well.\n",
    "            (4) Mask the ground truth labels with the annotation mask and return\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction : Tensor\n",
    "            predictions of segmentation model with\n",
    "            shape n_classes x L x W x H\n",
    "\n",
    "        annotation_mask : Tensor\n",
    "            current annotation, shape n_classes x L x W x H\n",
    "\n",
    "        n_samples : int\n",
    "            number of samples per slice before brushing\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interaction_map : Tensor\n",
    "            new annotations, shape n_classes x L x W x H\n",
    "\n",
    "        \"\"\"\n",
    "        n_classes = prediction.shape[0]\n",
    "\n",
    "        # calculate inverse class frequencies\n",
    "        inverse_size_weights = self.gt.mean((1,2,3)).sum() / self.gt.mean((1,2,3)).reshape((n_classes,1,1,1))\n",
    "\n",
    "        # calculate mask for available voxels\n",
    "        available_voxels = 1 - torch.any(annotation_mask, dim=0, keepdim=True) * 1\n",
    "    \t\n",
    "        annotated_voxels = torch.any(annotation_mask, axis=0)\n",
    "        brain_not_annoated_mask = brain_mask & ~annotated_voxels\n",
    "        x = torch.zeros((5,145,145,145))\n",
    "        x[:, brain_not_annoated_mask] = 1     # 5, 145, 145, 145\n",
    "        random_mask = torch.zeros((145,145,145))\n",
    "        random_mask[brain_not_annoated_mask] = 1   # 145, 145, 145\n",
    "\n",
    "        if mode == 'single_slice':\n",
    "            #np.random.seed(seed)\n",
    "            random_axis = np.random.randint(0,3)\n",
    "            match random_axis:\n",
    "                case 0:\n",
    "                    slice_sums = torch.sum(random_mask, axis=(1,2))\n",
    "                case 1:\n",
    "                    slice_sums = torch.sum(random_mask, axis=(0,2))\n",
    "                case 2:\n",
    "                    slice_sums = torch.sum(random_mask, axis=(0,1))\n",
    "\n",
    "            valid_slice_indices = torch.where(slice_sums >= n_samples)[0]\n",
    "            random_slice_index = np.random.choice(valid_slice_indices)     \n",
    "\n",
    "            ax = random_axis\n",
    "            slc = random_slice_index\n",
    "            #print(ax, slc)\n",
    "            data_location = (ax, slc)\n",
    "            selection = [slice(None)] + [slice(None)] * 3\n",
    "            selection[ax + 1] = slc\n",
    "\n",
    "            random_selection = x[selection]\n",
    "            t_selection = self.gt[selection]\n",
    "\n",
    "            samples = self.sample_random_candidate_voxels(random_selection, t_selection, n_samples)\n",
    "            brushed_mask = self._slice_add_neighbors(samples, t_selection)\n",
    "\n",
    "            interaction_map = torch.zeros_like(self.gt, dtype=torch.int64)\n",
    "            interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "\n",
    "        elif mode == 'per_class':\n",
    "            data_location = []\n",
    "            interaction_map = torch.zeros_like(self.gt, dtype=torch.int64)\n",
    "            for c in range(n_classes):\n",
    "                slice_sums = self._sum_l1_per_slice(random_mask)\n",
    "                random_axis = np.random.randint(0,3)\n",
    "                match random_axis:\n",
    "                    case 0:\n",
    "                        slice_sums = slice_sums[0]\n",
    "                    case 1:\n",
    "                        slice_sums = slice_sums[1]\n",
    "                    case 2:\n",
    "                        slice_sums = slice_sums[2]\n",
    "                \n",
    "                valid_slice_indices = torch.argwhere(slice_sums > 0).flatten()    # NOTE: oder größer gleich n_samples?\n",
    "                random_slice_index = np.random.choice(valid_slice_indices)\n",
    "\n",
    "                ax = random_axis\n",
    "                slc = random_slice_index\n",
    "\n",
    "                selection = [slice(None)] + [slice(None)] * 3\n",
    "                selection[ax + 1] = slc\n",
    "                random_selection = x[selection]\n",
    "                t_selection = self.gt[selection]\n",
    "\n",
    "                samples = self.sample_random_candidate_voxels(random_selection, t_selection, n_samples, seed=seed)\n",
    "                brushed_mask = self._slice_add_neighbors(samples, t_selection)\n",
    "                interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "                \n",
    "        return interaction_map.float() # , selection\n",
    "\n",
    "\n",
    "    # def novelty_refinement_annotation(\n",
    "    #     self,\n",
    "    #     annotation_mask: Tensor, \n",
    "    #     novelty_map: Tensor,\n",
    "    #     n_samples: int, \n",
    "    #     mode: int = 'per_class', \n",
    "    #     pos_weight: float = 1, \n",
    "    #     seed: int = 42\n",
    "    # ):\n",
    "    \n",
    "    #     n_classes = annotation_mask.shape[0]\n",
    "\n",
    "    #     # calculate inverse class frequencies\n",
    "    #     inverse_size_weights = self.gt.mean((1,2,3)).sum() / self.gt.mean((1,2,3)).reshape((n_classes,1,1,1))\n",
    "\n",
    "    #     # calculate mask for available voxels\n",
    "    #     #available_voxels = 1 - annotation_mask.float() # alte Version\n",
    "    #     available_voxels = 1 - torch.any(annotation_mask, dim=0, keepdim=True) * 1\n",
    "\n",
    "    #     # mask novelty score for all voxels that are already annotated\n",
    "    #     diff = novelty_map * available_oxels\n",
    "\n",
    "    #     if mode == 'per_class':\n",
    "    #         data_location = []\n",
    "    #         interaction_map = torch.zeros_like(self.gt, dtype=torch.int64)\n",
    "    #         for c in range(n_classes):\n",
    "    #             cweight = torch.eye(n_classes)[c].view(n_classes, 1, 1, 1)\n",
    "    #             diff_norm = torch.norm(diff * cweight, p=1, dim=0) # 145, 145, 145, binär\n",
    "\n",
    "    #             # 1.1) calc sum over l1 norms, e.g. for the l1 norms for segmentation predictions\n",
    "    #             slice_sums = self._sum_l1_per_slice(diff_norm)  # 3, 145\n",
    "\n",
    "    #             # 1.2) order slices in descending order by their sum\n",
    "    #             axis, indices = self._order_slices_by_sum(slice_sums)\n",
    "\n",
    "    #             # 2.0) select slice with highest importance weight over all axes\n",
    "    #             random_selection = np.random.randint(0,6)\n",
    "    #             ax  = axis[0]\n",
    "    #             slc = indices[0]\n",
    "                \n",
    "    #             selection = [slice(None)] + [slice(None)] * 3\n",
    "    #             selection[ax + 1] = slc\n",
    "\n",
    "    #             # 2.1) calculate number of samples for each class from a raw difference slice\n",
    "    #             diff_selection  = diff[selection]\n",
    "    #             t_selection     = self.gt[selection]\n",
    "                \n",
    "    #             slice_sample_weights = inverse_size_weights * cweight * n_classes / (n_classes+1) * pos_weight + \\\n",
    "    #                                 inverse_size_weights * (1-cweight) / ( (n_classes+1) * (n_classes-1) )\n",
    "                \n",
    "    #             n_class_samples = self._slice_samples_per_class(t_selection, slice_sample_weights, n_samples)\n",
    "    #             #print(n_class_samples, n_samples)\n",
    "                \n",
    "    #             # 2.2) for each class, sample from high novelty areas as often as specified in n_class_samples\n",
    "    #             class_samples = self._sample_candidate_voxels(diff_selection, t_selection, n_class_samples=n_class_samples, seed=seed)\n",
    "\n",
    "    #             # 2.3) brush all samples with maximum brush from list of brushes (deprecated. Brush size is usually 1)\n",
    "    #             brushed_mask = self._slice_add_neighbors(class_samples, t_selection)\n",
    "\n",
    "    #             # 2.4) create interaction map to return\n",
    "    #             interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "    #             data_location.append((axis[0], indices[0]))\n",
    "                \n",
    "    #     return interaction_map.float() # , selection\n",
    "\n",
    "\n",
    "\n",
    "        # \"\"\" Finds the slice with the worst prediction across all three axis and \n",
    "        #     annotates parts of it. The annotation happens in multiple steps:\n",
    "        #     (1) mask all voxels that are already annotated with annotation_mask\n",
    "        #     (2) Sample n_samples many seeding points and save their position in\n",
    "        #         an annotation mask\n",
    "        #     (3) Apply the largest quadratic brush (from a given range) to each seed\n",
    "        #         for which all affected voxels are foreground and add them to\n",
    "        #         the annotation mask as well.\n",
    "        #     (4) Mask the ground truth labels with the annotation mask and return\n",
    "\n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # prediction : Tensor\n",
    "        #     predictions of segmentation model with\n",
    "        #     shape n_classes x L x W x H\n",
    "\n",
    "        # annotation_mask : Tensor\n",
    "        #     current annotation, shape n_classes x L x W x H\n",
    "\n",
    "        # n_samples : int\n",
    "        #     number of samples per slice before brushing\n",
    "\n",
    "        # Returns\n",
    "        # -------\n",
    "        # interaction_map : Tensor\n",
    "        #     new annotations, shape n_classes x L x W x H\n",
    "\n",
    "        # \"\"\"\n",
    "        # # n_classes = prediction.shape[0]\n",
    "\n",
    "        # # calculate inverse class frequencies\n",
    "        # # inverse_size_weights = self.gt.mean((1,2,3)).sum() / self.gt.mean((1,2,3)).reshape((n_classes,1,1,1))\n",
    "\n",
    "        # # calculate mask for available voxels\n",
    "        # #available_voxels = 1 - annotation_mask.float() # alte Version\n",
    "        # available_voxels = 1 - torch.any(annotation_mask, dim=0, keepdim=True) * 1\n",
    "\n",
    "        # # calculate novelty scores within the available voxels\n",
    "        # diff = novelty_map * available_voxels\n",
    "\n",
    "        # # 1.1) calc slice wise scores as sum over all pixels within the slice\n",
    "        # slice_sums = self._sum_l1_per_slice(diff)\n",
    "\n",
    "        # # 1.2) order slices in descending order by their sum\n",
    "        # axis, indices = self._order_slices_by_sum(slice_sums)\n",
    "\n",
    "        # # 2.0) select slice with highest importance weight over all axes\n",
    "        # random_selection = np.random.randint(0,6)\n",
    "        # ax  = axis[0]\n",
    "        # slc = indices[0]\n",
    "        # data_location = (ax, slc)\n",
    "        # selection = [slice(None)] + [slice(None)] * 3\n",
    "        # selection[ax + 1] = slc\n",
    "\n",
    "        # # 2.1) calculate number of samples for each class from a raw difference slice\n",
    "        # diff_selection  = diff[selection]\n",
    "        # t_selection     = self.gt[selection]\n",
    "        # # n_class_samples = self._slice_samples_per_class(diff_selection, inverse_size_weights, n_samples)\n",
    "        # #print(n_class_samples.sum())\n",
    "\n",
    "        # # 2.2) for each class, sample from false negatives as often as specified in n_class_samples\n",
    "        # class_samples = self._sample_candidate_voxels(diff_selection, t_selection, n_class_samples=n_class_samples, seed=seed)\n",
    "\n",
    "        # # 2.3) brush all samples with maximum brush from list of brushes\n",
    "        # brushed_mask = self._slice_add_neighbors(class_samples, t_selection)\n",
    "\n",
    "        # # 2.4) create interaction map to return\n",
    "        # interaction_map = torch.zeros_like(self.gt, dtype=torch.int64)\n",
    "        # interaction_map[selection] = torch.bitwise_or(interaction_map[selection], brushed_mask)\n",
    "        # # interaction_map[selection] = ((interaction_map[selection].sum(0) * t_selection) > 0) * 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        subject_id: str,\n",
    "        cfg, \n",
    "        modality='reconstruction', \n",
    "        mode='train', \n",
    "        to_gpu=True, \n",
    "        init='three_slices', \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg          = OmegaConf.to_container(cfg)\n",
    "        self.axis         = cfg.data.axis\n",
    "        self.labels       = cfg.data.labels\n",
    "        self.device       = cfg.rank\n",
    "        self.modality     = modality\n",
    "        self.mode         = mode\n",
    "        self.to_gpu       = to_gpu\n",
    "        self.init         = init\n",
    "\n",
    "        self.data_path = os.path.join(\n",
    "            cfg.data.data_dir, \n",
    "            str(subject_id), \n",
    "            \"Diffusion\", \n",
    "            \"data.nii.gz\"\n",
    "        )\n",
    "        self.data_in = torch.tensor(nib.load(self.data_path).get_fdata()).float()\n",
    "\n",
    "        self.mask_path = os.path.join(\n",
    "            cfg.data.data_dir, \n",
    "            str(subject_id), \n",
    "            \"Diffusion\", \n",
    "            \"nodif_brain_mask.nii.gz\"\n",
    "        )\n",
    "        self.brain_mask = torch.tensor(\n",
    "            nib.load(self.mask_path).get_fdata(), dtype=torch.bool)\n",
    "        \n",
    "        self.tract_path = os.path.join(\n",
    "            cfg.data.data_dir, \n",
    "            str(subject_id), \n",
    "            \"tracts_masks\"\n",
    "        )\n",
    "        self.get_tract_masks(self.labels)\n",
    "\n",
    "        if self.axis == 'coronal':\n",
    "            self.data_in = self.data_in.permute(1,3,2,0).rot90(2, dims=[1,2])[14:159]\n",
    "            self.brain_mask = self.brain_mask.permute(1,2,0).rot90(2, dims=[1,2])[14:159]\n",
    "            self.label = self.label.permute(0,2,3,1).rot90(2, dims=[2,3])[:,14:159]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only coronal axis is supported at the moment\")\n",
    "\n",
    "        # if cfg['log']:\n",
    "        #    wandb.config.update({'labels': cfg['labels']})\n",
    "            \n",
    "        self.user = UserModel(self.label, cfg)\n",
    "            \n",
    "        # [classes, B, H, W]\n",
    "        self.annotations = None\n",
    "\n",
    "        # [B, 1, H, W]\n",
    "        self.weight = None\n",
    "\n",
    "        self.pos_weight = (\n",
    "            len(self.labels)*self.brain_mask.sum() - \\\n",
    "            self.label.sum((1,2,3))[None, :, None, None]\n",
    "        ) / self.label.sum((1,2,3))[None, :, None, None]\n",
    "        \n",
    "        if self.to_gpu:\n",
    "            self.data_in    = self.data_in.to(self.device)\n",
    "            self.label      = self.label.to(self.device)\n",
    "            self.brain_mask = self.brain_mask.to(self.device) \n",
    "            self.pos_weight = self.pos_weight.to(self.device) \n",
    "\n",
    "\n",
    "    def get_tract_masks(\n",
    "        self,\n",
    "        labels: List[str]\n",
    "    ) -> Tensor:\n",
    "        self.label = None\n",
    "        for tract in self.labels:\n",
    "            \n",
    "            if f'{tract}.nii.gz' in os.listdir(self.tract_path):\n",
    "                tract_mask = torch.tensor(\n",
    "                    nib.load(os.path.join(self.tract_path, f'{tract}.nii.gz')).get_fdata()\n",
    "                ).long()\n",
    "                if self.label is None:\n",
    "                    self.label = tract_mask.unsqueeze(0)\n",
    "                else:\n",
    "                    self.label = torch.cat([\n",
    "                        self.label, \n",
    "                        tract_mask.unsqueeze(0)\n",
    "                    ], dim=0)\n",
    "\n",
    "            # class other is handled below\n",
    "            elif tract == 'Other':\n",
    "                continue\n",
    "            else:\n",
    "                # left and right are different classes but the raw data makes\n",
    "                # even more distictions we don't care for\n",
    "                if 'left' in tract or 'right' in tract:\n",
    "                    tract = tract.split('.')[0]\n",
    "                    tract_parts = tract.split('_')\n",
    "                    tract, side = tract_parts[0], tract_parts[-1]\n",
    "                    # tract, side = tract.split('_')\n",
    "                else:\n",
    "                    side = ''\n",
    "                tract_files = [\n",
    "                    f for f in os.listdir(self.tract_path)\n",
    "                    if tract in f and side in f\n",
    "                ]\n",
    "\n",
    "                tract_mask = None\n",
    "                for f in tract_files:\n",
    "                    tract_mask_tmp = torch.tensor(\n",
    "                        nib.load(os.path.join(self.tract_path, f)).get_fdata()\n",
    "                    ).long()\n",
    "                    if tract_mask is None:\n",
    "                        tract_mask = tract_mask_tmp\n",
    "                    else:\n",
    "                        tract_mask = torch.bitwise_or(\n",
    "                            tract_mask, tract_mask_tmp\n",
    "                        )\n",
    "\n",
    "                if self.label is None:\n",
    "                    self.label = tract_mask.unsqueeze(0)\n",
    "                else:\n",
    "                    self.label = torch.cat([\n",
    "                        self.label, \n",
    "                        tract_mask.unsqueeze(0)\n",
    "                    ], dim=0)\n",
    "\n",
    "\n",
    "        other = self.brain_mask * ~torch.any(self.label, dim=0)\n",
    "        self.label = torch.cat([\n",
    "            other.unsqueeze(0),\n",
    "            self.label, \n",
    "        ], dim=0)\n",
    "           \n",
    "        \n",
    "    def set_mode(self, mode) -> None:\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def set_modality(self, modality) -> None:\n",
    "        self.modality = modality     \n",
    "        \n",
    "        \n",
    "    def initial_annotation(\n",
    "        self, \n",
    "        seed=42\n",
    "    ) -> Tensor:\n",
    "        return self.user.initial_annotation(\n",
    "            #self.label.detach().cpu(),\n",
    "            self.cfg[\"init_voxels\"],\n",
    "            init=self.init, \n",
    "            seed=seed\n",
    "        )\n",
    "    \n",
    "\n",
    "    def random_refinement_annotation(\n",
    "        self,\n",
    "        prediction, \n",
    "        seed=42\n",
    "    ) -> Tensor:\n",
    "        \n",
    "        if self.init == 'per_class':\n",
    "            mode = 'per_class'\n",
    "            \n",
    "        elif self.init == 'three_slices':\n",
    "            mode = 'single_slice'\n",
    "\n",
    "        return self.user.random_refinement_annotation(\n",
    "            prediction, \n",
    "            self.annotations.detach().cpu(),\n",
    "            self.brain_mask.detach().cpu(),\n",
    "            self.cfg[\"refinement_voxels\"],\n",
    "            mode=mode,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "\n",
    "    def refinement_annotation(\n",
    "        self, \n",
    "        prediction, \n",
    "        uncertainty_map=None, \n",
    "        random=False, \n",
    "        seed=42\n",
    "    ) -> Tensor:\n",
    "        \n",
    "        if self.init == 'per_class':\n",
    "            mode = 'per_class'\n",
    "            \n",
    "        elif self.init == 'three_slices':\n",
    "            mode = 'single_slice'\n",
    "        \n",
    "        elif random:\n",
    "            return self.user.random_refinement_annotation(\n",
    "                prediction, \n",
    "                self.annotations.detach().cpu(),\n",
    "                self.brain_mask.detach().cpu(),\n",
    "                self.cfg[\"refinement_voxels\"],\n",
    "                mode=mode,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return self.user.refinement_annotation(\n",
    "                prediction,\n",
    "                #self.label.detach().cpu(),\n",
    "                self.annotations.detach().cpu(),\n",
    "                uncertainty_map,\n",
    "                self.cfg[\"refinement_voxels\"],\n",
    "                mode=mode,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "\n",
    "    def update_annotation(self, annotations) -> None:\n",
    "        assert(annotations.data.type() == 'torch.FloatTensor')\n",
    "\n",
    "        if self.to_gpu:\n",
    "            annotations = annotations.to(self.device)\n",
    "\n",
    "        if self.annotations is None:\n",
    "            self.annotations = annotations\n",
    "        else:\n",
    "            self.annotations += annotations\n",
    "            self.annotations  = torch.clamp(self.annotations, 0, 1)\n",
    "            \n",
    "        self.weight = (self.annotations.sum(0) > 0).unsqueeze(1).float()\n",
    "        self.pos_weight  = (self.weight.sum() - self.annotations.sum((1,2,3))[None, :, None, None])\n",
    "#             self.pos_weight  = (1 - self.annotations.sum((1,2,3))[None, :, None, None])\n",
    "        self.pos_weight /= self.annotations.sum((1,2,3))[None, :, None, None]\n",
    "\n",
    "    \n",
    "    def clear_annotation(self) -> None:\n",
    "        self.annotations = None\n",
    "        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data_in.shape[0]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index) -> dict:\n",
    "\n",
    "        input_ = self.data_in[index]\n",
    "\n",
    "        if self.modality == 'reconstruction':\n",
    "            target = self.data_in[index].detach().clone()\n",
    "            weight = 1.\n",
    "            \n",
    "        elif self.modality == 'segmentation':\n",
    "        \n",
    "            if self.mode == 'train':\n",
    "                target = self.annotations[:, index].detach()\n",
    "            elif self.mode == 'validate':\n",
    "                target = self.label[:, index].detach()\n",
    "    \n",
    "\n",
    "            weight = self.weight[index]\n",
    "\n",
    "        mask = self.brain_mask[index]\n",
    "        \n",
    "        return {\n",
    "            'input':  input_,\n",
    "            'target': target,\n",
    "            'weight': weight, # may needs unsqueeze(0) in validate\n",
    "            'mask':   mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = {\n",
    "#     # CONFIG\n",
    "#     'name': 'location-unsupervised',\n",
    "#     'project': 'IDVR-localization_pretrain',\n",
    "#     'log': False,\n",
    "#     'rank': 0,\n",
    "#     'axis': 'coronal',\n",
    "#     # 'labels': [\n",
    "#     #     \"Other\", \n",
    "#     #     \"CG\", \n",
    "#     #     \"CST\", \n",
    "#     #     \"FX\", \n",
    "#     #     \"CC\"\n",
    "#     # ],\n",
    "#     'labels': [\n",
    "#         \"Other\", \n",
    "#         \"IFO_left\", \n",
    "#         \"IFO_right\", \n",
    "#         \"ILF_left\",\n",
    "#         \"ILF_right\", \n",
    "#         \"SLF_left\", \n",
    "#         \"SLF_right\"\n",
    "#     ],\n",
    "#     'slice_selection': 'mean',\n",
    "#     'voxel_selection': 'mean',\n",
    "    \n",
    "#     # DATA\n",
    "#     'data_dir': '../../../../../data/hcp',\n",
    "#     'data_path': '../../../data/784565/Diffusion/data.nii',\n",
    "#     'active_mask_path': '../../../data/784565/Diffusion/nodif_brain_mask.nii.gz',\n",
    "    \n",
    "#     # SELF SUPERVISED PRE-TRAINING\n",
    "#     's_n_epochs': 20,\n",
    "#     's_batch_size': 16, # default: 8\n",
    "#     's_lr': 5e-4, #1e-4, 1e-5        \n",
    "    \n",
    "#     # TRAINING WITH WEAK SUPERVISION\n",
    "#     'p_n_epochs': 100,\n",
    "#     'w_n_epochs': 10,\n",
    "#     'w_batch_size': 2,\n",
    "#     'w_lr': 5e-4,    #5e-5 \n",
    "#     'w_eval_freq': 100,\n",
    "    \n",
    "#     # RANDOM FOREST\n",
    "#     'min_samples_leaf': 8,\n",
    "    \n",
    "#     # USER MODEL\n",
    "#     'init_voxels': 200,\n",
    "#     'refinement_voxels': 200,\n",
    "#     'num_interactions': 10,\n",
    "# }\n",
    "\n",
    "cfg = OmegaConf.load('../configs/eval.yaml')\n",
    "\n",
    "dataset = EvalDataset(\n",
    "    subject_id=729254, \n",
    "    cfg=cfg, \n",
    "    modality='reconstruction',\n",
    "    to_gpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([97,  0,  2,  0,  1], dtype=torch.int32)\n",
      "tensor([ 1, 97,  1,  0,  1], dtype=torch.int32)\n",
      "tensor([ 0,  0, 98,  2,  0], dtype=torch.int32)\n",
      "tensor([  0,   0,   0, 100,   0], dtype=torch.int32)\n",
      "tensor([ 1,  4,  3,  0, 92], dtype=torch.int32)\n",
      "number of annotations: 663.0\n"
     ]
    }
   ],
   "source": [
    "dataset.user = UserModel(dataset.label, cfg)\n",
    "dataset.init = 'per_class'\n",
    "# currently, there are no annotations. We can also enforce this with clear_annotations() at any point\n",
    "dataset.clear_annotation()\n",
    "# get initial annotations\n",
    "annot = dataset.initial_annotation(seed=42)\n",
    "# and update the dataset\n",
    "dataset.update_annotation(annot)\n",
    "print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model\n",
    "model, state_dict = get_model(\n",
    "    cfg=cfg,\n",
    "    return_state_dict=True\n",
    ")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the layer you want the features from. This is usually the encoder output.\n",
    "f_layer = 'encoder'\n",
    "# Init the feature extractor. Have a look at PyTorchs Hook functionality.\n",
    "extractor = FeatureExtractor(model, layers=[f_layer])\n",
    "# Cache all features for a dataset and reformat/move to numpy for random forest stuff\n",
    "hooked_results  = extractor(dataset)\n",
    "features = hooked_results[f_layer]\n",
    "features = features.permute(0,2,3,1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, prediction, uncertainty_maps, uncertainty_per_class_maps = evaluate_RF(\n",
    "    dataset, \n",
    "    features, \n",
    "    cfg,\n",
    "    uncertainty_measures=['entropy-truth']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Other_precision': array(0.85041994, dtype=float32),\n",
       " 'Other_recall': array(0.30600485),\n",
       " 'Other_f1': array(0.45006883),\n",
       " 'CG_precision': array(0.24978456, dtype=float32),\n",
       " 'CG_recall': array(0.65688817),\n",
       " 'CG_f1': array(0.36194695),\n",
       " 'CST_precision': array(0.2541128, dtype=float32),\n",
       " 'CST_recall': array(0.52359891),\n",
       " 'CST_f1': array(0.34217429),\n",
       " 'FX_precision': array(0.04507279, dtype=float32),\n",
       " 'FX_recall': array(0.71396894),\n",
       " 'FX_f1': array(0.08480468),\n",
       " 'CC_precision': array(0.40540728, dtype=float32),\n",
       " 'CC_recall': array(0.68015717),\n",
       " 'CC_f1': array(0.50801798),\n",
       " 'Avg_prec_tracts': array(0.23859435, dtype=float32),\n",
       " 'Avg_recall_tracts': array(0.6436533),\n",
       " 'Avg_f1_tracts': array(0.32423597)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, prediction, uncertainty_maps, uncertainty_per_class_maps = evaluate_RF(\n",
    "    dataset, \n",
    "    features, \n",
    "    cfg,\n",
    "    uncertainty_measures=['entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Other_precision': array(0.85041994, dtype=float32),\n",
       " 'Other_recall': array(0.30600485),\n",
       " 'Other_f1': array(0.45006883),\n",
       " 'CG_precision': array(0.24978456, dtype=float32),\n",
       " 'CG_recall': array(0.65688817),\n",
       " 'CG_f1': array(0.36194695),\n",
       " 'CST_precision': array(0.2541128, dtype=float32),\n",
       " 'CST_recall': array(0.52359891),\n",
       " 'CST_f1': array(0.34217429),\n",
       " 'FX_precision': array(0.04507279, dtype=float32),\n",
       " 'FX_recall': array(0.71396894),\n",
       " 'FX_f1': array(0.08480468),\n",
       " 'CC_precision': array(0.40540728, dtype=float32),\n",
       " 'CC_recall': array(0.68015717),\n",
       " 'CC_f1': array(0.50801798),\n",
       " 'Avg_prec_tracts': array(0.23859435, dtype=float32),\n",
       " 'Avg_recall_tracts': array(0.6436533),\n",
       " 'Avg_f1_tracts': array(0.32423597)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(n_annots: list, f1_scores: list):\n",
    "    print(f'Iteration | # Annotations | F1 Score')\n",
    "    print(f'----------|---------------|---------')\n",
    "    for i, (n, f1) in enumerate(zip(n_annots, f1_scores)):\n",
    "        if i in [0, 1, 2, 3, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54]:\n",
    "            print(f'{i+1:>9} | {int(n):>13} | {f1:.4f}')\n",
    "\n",
    "\n",
    "def re_init_dataset():\n",
    "    dataset.clear_annotation()\n",
    "    annot = dataset.initial_annotation(seed=42)\n",
    "    dataset.update_annotation(annot)\n",
    "\n",
    "\n",
    "def train(\n",
    "    method: str, \n",
    "    n_epochs: int, \n",
    "    measures: List[str]\n",
    "):\n",
    "    re_init_dataset()\n",
    "    print(f'Selection using {method}')\n",
    "    #print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    scores, rf_prediction, unc, unc_pc = evaluate_RF(dataset, features, cfg, measures)\n",
    "    # scores, rf_prediction, unc, unc_pc = evaluate_RF_with_uncertainty(dataset, features, cfg, measures)\n",
    "    #scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "    print(f\"Number of initial annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "    print()\n",
    "    n_annots =       []\n",
    "    annots =         []\n",
    "    f1_scores =      []\n",
    "    rf_predictions = []\n",
    "    uncs_pc =        []\n",
    "    uncs =           []\n",
    "\n",
    "\n",
    "    for i in tqdm(range(n_epochs), desc='User interaction', unit='iteration'):\n",
    "        #print(f\"Iteration {i+1}\")\n",
    "        if method == 'random':\n",
    "            annot = dataset.refinement_annotation(prediction=rf_prediction, random=True, seed=42)\n",
    "        elif method == 'ground-truth':\n",
    "            annot = dataset.refinement_annotation(prediction=rf_prediction, seed=42)\n",
    "        else:\n",
    "            annot = dataset.refinement_annotation(prediction=rf_prediction, uncertainty_map=unc_pc[method], seed=42)\n",
    "            #annot = dataset.uncertainty_refinement_annotation(prediction=rf_prediction, uncertainty_map=unc_pc[method], seed=42)\n",
    "\n",
    "        dataset.update_annotation(annot)\n",
    "        \n",
    "        annots.append(dataset.annotations.detach().cpu())\n",
    "        n_annots.append(dataset.annotations.detach().cpu().sum().item())\n",
    "        #print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "        scores, rf_prediction, unc, unc_pc = evaluate_RF(dataset, features, cfg, measures)\n",
    "        #scores, rf_prediction, unc, unc_pc = evaluate_RF_with_uncertainty(dataset, features, cfg, measures)\n",
    "        #scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "        rf_predictions.append(rf_prediction)\n",
    "        #uncs_pc.append(unc_pc)\n",
    "        #uncs.append(unc)\n",
    "        f1_scores.append(scores['Avg_f1_tracts'].item())\n",
    "        #print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "    \n",
    "    print_results(n_annots, f1_scores)\n",
    "    return n_annots, annots, f1_scores, rf_predictions, uncs_pc, uncs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
