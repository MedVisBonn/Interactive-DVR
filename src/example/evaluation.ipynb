{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b8d26-e301-49fd-b2bb-a185310027f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Golo\\miniconda3\\envs\\BA\\Lib\\site-packages\\torch\\utils\\_contextlib.py:125: UserWarning: Decorating classes is deprecated and will be disabled in future versions. You should only decorate functions or methods. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\n",
      "  warnings.warn(\"Decorating classes is deprecated and will be disabled in \"\n"
     ]
    }
   ],
   "source": [
    "#import os, sys\n",
    "#from time import time\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "#from torch.cuda.amp import GradScaler, autocast\n",
    "#from torch.utils.data._utils.collate import default_collate\n",
    "#import copy\n",
    "#from time import time\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import AEDataset\n",
    "from trainer import Trainer, WeakSupervisionTrainer\n",
    "from model import DualBranchAE\n",
    "from utils import *\n",
    "from losses import MSELoss\n",
    "from pretrainer import PreTrainer\n",
    "from torchmetrics.classification import BinaryROC\n",
    "from scipy import ndimage\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc02d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload changes in .py files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211305c3-326d-4c47-bbec-5087737b3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb22710-307a-4146-8bdd-57bba48aad1e",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We currently work on the HPC data and within this, we built two different segmentation tasks. Further details are in the paper https://cg.cs.uni-bonn.de/backend/v1/files/publications/torayev-vcbm2020.pdf. Neither the whole dataset nor the model are in this repo. We will set you up once you started your work and give your access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c199b3f-5e9b-4d46-acdb-3770417afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tasks are used is handled by \"set\". 1 is a binary task for debugging, 2 is multi-class \n",
    "# and so is 3 but with asymmetric classes w.r.t. the saggital plane (harder). Details for \n",
    "# set 2 and 3 are in the paper.\n",
    "# 'modality' handles the target provided by __getitem__. Options are reconstruction and segmentation.\n",
    "# When segmentation is selected, the labels are taken from the annotations attribute. This is also where\n",
    "# the user-model interacts with the dataset. Ground truth masks are in the label attribute. All other parameters are\n",
    "# from past experiments and alter the behaviour. This project has been around for a while, so some are not used anymore.\n",
    "\n",
    "# normalize is usually set to true. Simply normalizes the input. Augment is legacy, we didn't have much success\n",
    "# with data augmentation. balance takes care of data balancing during a batch. Some classes are under-\n",
    "# represented so we show them to the model more often. It helps quite a bit during training so consider \n",
    "# integrating it. We can talk about how this is done in detail once you start. init defines how the user-model behaves. \n",
    "# We considered different behaviours w.r.t. to annotation style and quantity and such. \n",
    "# To_gpu moves ALL data to GPU. Since we only work on a single volume (i.e. couple hundred slices) \n",
    "# we move everything to GPU and avoid latency in dataloading. Takes a hefty chunk out of the VRAM though \n",
    "# but makes things faster.\n",
    "\n",
    "# Feel free to re-write anything you want. This is partly dated code that could use a re-write anyways.\n",
    "\n",
    "# Example:\n",
    "# make a config first. This handles globals and is used through-out the script. Many things that were tried in\n",
    "# experiments later have not yet made it into the config, but most have.\n",
    "\n",
    "cfg = {\n",
    "    # CONFIG\n",
    "    'name': 'location-unsupervised',\n",
    "    'project': 'IDVR-localization_pretrain',\n",
    "    'log': False,\n",
    "    'rank': 0,\n",
    "    \n",
    "    # DATA\n",
    "    'data_dir': '../../../data/784565/Diffusion/',\n",
    "    'data_path': '../../../data/784565/Diffusion/data.nii',\n",
    "    'active_mask_path': '../../../data/784565/Diffusion/nodif_brain_mask.nii.gz',\n",
    "    \n",
    "    # SELF SUPERVISED PRE-TRAINING\n",
    "    's_n_epochs': 20,\n",
    "    's_batch_size': 16, # default: 8\n",
    "    's_lr': 5e-4, #1e-4, 1e-5        \n",
    "    \n",
    "    # TRAINING WITH WEAK SUPERVISION\n",
    "    'p_n_epochs': 100,\n",
    "    'w_n_epochs': 10,\n",
    "    'w_batch_size': 2,\n",
    "    'w_lr': 5e-4,    #5e-5 \n",
    "    'w_eval_freq': 100,\n",
    "    \n",
    "    # RANDOM FOREST\n",
    "    'min_samples_leaf': 8,\n",
    "    \n",
    "    # USER MODEL\n",
    "    'init_voxels': 200,\n",
    "    'refinement_voxels': 200,\n",
    "    'num_interactions': 10,\n",
    "    'brush' : False,\n",
    "    'slice_selection' : 'mean',\n",
    "    'voxel_selection' : 'max', \n",
    "    'voxel_distribution' : 'uniform',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66666ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape torch.Size([145, 145])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 1, 1])\n",
      "Shape torch.Size([145, 145])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 1, 1])\n",
      "Shape torch.Size([145, 145])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 1, 1])\n",
      "Shape torch.Size([145, 145])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 1, 1])\n",
      "Shape torch.Size([145, 145])\n",
      "tensor([0., 1.])\n",
      "tensor([0., 1.])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 1, 1])\n",
      "number of annotations: 1267.0\n",
      "dict_keys(['input', 'target', 'weight', 'mask'])\n"
     ]
    }
   ],
   "source": [
    "# we set balance to true. This also effects the dataloader later\n",
    "balance = True\n",
    "dataset = AEDataset(cfg, modality='segmentation', normalize=True,\n",
    "                    set=2, augment=False, balance=balance, init='per_class', to_gpu=False)\n",
    "# dataset = AEDataset(cfg, modality='segmentation', normalize=True,\n",
    "                    # set=2, augment=False, balance=balance, init='three_slices', to_gpu=False)\n",
    "\n",
    "# currently, there are no annotations. We can also enforce this with clear_annotations() at any point\n",
    "dataset.clear_annotation()\n",
    "# get initial annotations\n",
    "annot = dataset.initial_annotation(seed=42)\n",
    "# and update the dataset\n",
    "dataset.update_annotation(annot)\n",
    "print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "\n",
    "# The dataset currently always provides 4 items. Input (the image), target (the input for reconstruction or \n",
    "# the annotations for segmentation), weights that mask out voxels which are not annotated for segmentation \n",
    "# and a brain mask for masking background during reconstruction\n",
    "item = dataset[0]\n",
    "print(item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be1b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1267.)\n",
      "(tensor([0., 1., 2.]), tensor([3047691,     601,     333]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset.annotations.sum())\n",
    "print(dataset.annotations.sum(dim=0).unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f901cd0-8b3b-4cb5-8b32-f89ab55a8595",
   "metadata": {},
   "source": [
    "# Model and Inference\n",
    "\n",
    "The overall pipeline is illustrated in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9e0d19-66c2-46ff-8f8c-bab0943aadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first, we do not have annotations but still need features for the Random Forest. So we pre-train \n",
    "# on a reconstruction task and later re-use the same Encoder (the part of the network that outputs our features),\n",
    "# simply replace the decoder and resume training. \n",
    "\n",
    "# init the model with segmentation decoder. Have a look at the source code for additional guidance. The dataset\n",
    "# updates the config to contain labels. We initialize with one channel per class.\n",
    "model = DualBranchAE(encoder    = 'dual',\n",
    "                     decoder    = 'segmentation',\n",
    "                     in_size    = 145,\n",
    "                     n_classes  = len(cfg['labels']),\n",
    "                     thresholds = 'learned', \n",
    "                     dropout = False, \n",
    "                     dropout_rate=0.5) #.to(cfg['rank'])\n",
    "\n",
    "# example model from one of the experiments\n",
    "#model_path = 'example_dual_xy_0_best.pt'\n",
    "model_path = 'models/Test_best.pt'\n",
    "#model_path = 'models/Dropout-0.5_best.pt'\n",
    "# model_path = 'models/Dropout-0.2_best.pt'\n",
    "\n",
    "# load the components\n",
    "checkpoint           = torch.load(model_path)\n",
    "model_state_dict     = checkpoint['model_dict']\n",
    "encoder_state_dict   = {k.replace('encoder.', ''): v for k, v in model_state_dict.items() if 'encoder' in k}\n",
    "# print(model_state_dict.keys())\n",
    "\n",
    "# copy encoder weights to model. Decoder weights remain as they are, initialized as random\n",
    "model.encoder.load_state_dict(encoder_state_dict, strict=True)\n",
    "\n",
    "# Define the dataloader. If we use balanced sampling in the dataset, we also need the custom balanced_collate \n",
    "# function in the dataloader. This handles the unusal batching logic.\n",
    "\n",
    "if balance:\n",
    "    loader  = DataLoader(dataset, \n",
    "                         batch_size=cfg['w_batch_size'], \n",
    "                         shuffle=True, \n",
    "                         drop_last=False, \n",
    "                         collate_fn=balanced_collate)\n",
    "else:\n",
    "    loader  = DataLoader(dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True, \n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c31b2b-2858-4b44-b99a-01a1caa794db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, we are interested in the Random Forest (RF) prediction based on\n",
    "# the CNN features. \n",
    "\n",
    "# write checkpoints for stuff that changes the behaviour of the dataset.\n",
    "# E.g. balancing changes the __getitem__ method and thus influences \n",
    "# evaluation. Turn it off and on later if needed.\n",
    "augment_checkpoint = dataset.augment\n",
    "balance_checkpoint = dataset.balance\n",
    "dataset.augment = False\n",
    "dataset.balance = False\n",
    "\n",
    "# define the layer you want the features from. This is usually the encoder output.\n",
    "f_layer = 'encoder'\n",
    "# Init the feature extractor. Have a look at PyTorchs Hook functionality.\n",
    "extractor = FeatureExtractor(model, layers=[f_layer])\n",
    "\n",
    "# Cache all features for a dataset and reformat/move to numpy for random forest stuff\n",
    "hooked_results  = extractor(dataset)\n",
    "features = hooked_results[f_layer]\n",
    "features  = features.permute(0,2,3,1).numpy()\n",
    "# In the utils file are a bunch of evaluation scripts, some are not used anymore.\n",
    "# This one provides F1 scores for the whole dataset based on all ground truth labels\n",
    "# and also the predictions themselve as given by the RF. We need them later to update the annotations with\n",
    "# the user model.\n",
    "\n",
    "# Turn dataset attributes to normal again\n",
    "dataset.augment = augment_checkpoint\n",
    "dataset.balance = balance_checkpoint\n",
    "\n",
    "\n",
    " #Now you can change the model and features to your liking and try again (e.g. via constrastive learning ;)).\n",
    " #The scores from the RF are the signal you need for evaluation, the rest is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96eff01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1., 2., 3.], dtype=torch.float64), tensor([800508,  43841,      1]))\n",
      "(tensor([0., 1., 2., 3.], dtype=torch.float64), tensor([644241, 156267,  43841,      1]))\n"
     ]
    }
   ],
   "source": [
    "x = dataset.label.permute(1,2,3,0)\n",
    "print(x[dataset.brain_mask].sum(dim=1).unique(return_counts=True))\n",
    "print(x[dataset.brain_mask][:, 1:].sum(dim=1).unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ca8f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([844350, 5])\n",
      "(tensor([0., 1.], dtype=torch.float64), tensor([200109, 644241]))\n"
     ]
    }
   ],
   "source": [
    "print(x[dataset.brain_mask].shape)\n",
    "print(x[dataset.brain_mask][:, :1].sum(dim=1).unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f0c37",
   "metadata": {},
   "source": [
    "# Uncertainty Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "437e0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(n_annots: list, f1_scores: list):\n",
    "    print(f'Iteration | # Annotations | F1 Score')\n",
    "    print(f'----------|---------------|---------')\n",
    "    for i, (n, f1) in enumerate(zip(n_annots, f1_scores)):\n",
    "        if i in [0, 1, 2, 3, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54]:\n",
    "            print(f'{i+1:>9} | {int(n):>13} | {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7757acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir brauchen für die erste Iteration mindestens 1 Annotation damit der RF funktioniert\n",
    "def re_init_dataset():\n",
    "    dataset.clear_annotation()\n",
    "    annot = dataset.initial_annotation(seed=42)\n",
    "    dataset.update_annotation(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8516af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(method: str, n_epochs: int, measures: List[str]):\n",
    "    re_init_dataset()\n",
    "    print(f'Selection using {method}')\n",
    "    #print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    scores, rf_prediction, unc, unc_pc = evaluate_RF(dataset, features, cfg, measures)\n",
    "    #scores, rf_prediction, unc, unc_pc = evaluate_RF_with_uncertainty(dataset, features, cfg, measures)\n",
    "    #scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "    print(f\"Number of initial annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "    print()\n",
    "    n_annots = []\n",
    "    annots = []\n",
    "    f1_scores = []\n",
    "    rf_predictions = []\n",
    "    uncs_pc = []\n",
    "    uncs = []\n",
    "\n",
    "\n",
    "    for i in tqdm(range(n_epochs), desc='User interaction', unit='iteration'):\n",
    "        #print(f\"Iteration {i+1}\")\n",
    "        if method == 'random':\n",
    "            annot = dataset.refinement_annotation(prediction=rf_prediction, random=True, seed=42)\n",
    "        elif method == 'ground-truth':\n",
    "            annot = dataset.refinement_annotation(prediction=rf_prediction, seed=42)\n",
    "        else:\n",
    "            annot = dataset.refinement_annotation(prediction=rf_prediction, uncertainty_map=unc_pc[method], seed=42)\n",
    "            #annot = dataset.uncertainty_refinement_annotation(prediction=rf_prediction, uncertainty_map=unc_pc[method], seed=42)\n",
    "\n",
    "        dataset.update_annotation(annot)\n",
    "        \n",
    "        annots.append(dataset.annotations.detach().cpu())\n",
    "        n_annots.append(dataset.annotations.detach().cpu().sum().item())\n",
    "        #print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "        scores, rf_prediction, unc, unc_pc = evaluate_RF(dataset, features, cfg, measures)\n",
    "        #scores, rf_prediction, unc, unc_pc = evaluate_RF_with_uncertainty(dataset, features, cfg, measures)\n",
    "        #scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "        rf_predictions.append(rf_prediction)\n",
    "        #uncs_pc.append(unc_pc)\n",
    "        #uncs.append(unc)\n",
    "        f1_scores.append(scores['Avg_f1_tracts'].item())\n",
    "        #print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "    \n",
    "    print_results(n_annots, f1_scores)\n",
    "    return n_annots, annots, f1_scores, rf_predictions, uncs_pc, uncs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad3342",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b72f5cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using ground-truth\n",
      "Number of initial annotations: 1267.0\n",
      "Average F1 score for RF after initial user interaction:    0.3617\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49d55bff39047948909c8170c4265d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User interaction:   0%|          | 0/20 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # Annotations | F1 Score\n",
      "----------|---------------|---------\n",
      "        1 |          1968 | 0.4362\n",
      "        2 |          2736 | 0.4664\n",
      "        3 |          3394 | 0.4949\n",
      "        4 |          4163 | 0.5281\n",
      "        5 |          4608 | 0.5536\n",
      "       10 |          8045 | 0.5885\n",
      "       15 |         10708 | 0.6634\n",
      "       20 |         13494 | 0.6734\n"
     ]
    }
   ],
   "source": [
    "ns, ans, f1s, rf_preds, uncs_pc, uncs = train(method='ground-truth', n_epochs=20, measures=['ground-truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391124d",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48d9b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using entropy\n",
      "Number of initial annotations: 1267.0\n",
      "Average F1 score for RF after initial user interaction:    0.3617\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d04de0f2874914970685285f99df07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User interaction:   0%|          | 0/20 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # Annotations | F1 Score\n",
      "----------|---------------|---------\n",
      "        1 |          2375 | 0.3956\n",
      "        2 |          3529 | 0.4189\n",
      "        3 |          4696 | 0.4208\n",
      "        4 |          5865 | 0.4296\n",
      "        5 |          6973 | 0.4333\n",
      "       10 |         12358 | 0.4836\n",
      "       15 |         17100 | 0.4927\n",
      "       20 |         21653 | 0.5037\n"
     ]
    }
   ],
   "source": [
    "ns_e, ans_e, f1s_e, rf_preds_e, uncs_pc_e, uncs_e = train(method='entropy', n_epochs=20, measures=['entropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d55839",
   "metadata": {},
   "source": [
    "### Spatial Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cd26406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using spatial-distance\n",
      "Number of initial annotations: 1267.0\n",
      "Average F1 score for RF after initial user interaction:    0.3617\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6472ec4926924ae582f96f0f4f581277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User interaction:   0%|          | 0/20 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # Annotations | F1 Score\n",
      "----------|---------------|---------\n",
      "        1 |          2310 | 0.4281\n",
      "        2 |          3318 | 0.4528\n",
      "        3 |          4261 | 0.4625\n",
      "        4 |          5165 | 0.4683\n",
      "        5 |          6110 | 0.4768\n",
      "       10 |          9922 | 0.5045\n",
      "       15 |         12894 | 0.5126\n",
      "       20 |         15837 | 0.5175\n"
     ]
    }
   ],
   "source": [
    "ns_sd, ans_sd, f1s_sd, rf_preds_sd, uncs_pc_sd, uncs_sd = train(method='spatial-distance', n_epochs=20, measures=['spatial-distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee0949",
   "metadata": {},
   "source": [
    "### Feature Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cac61b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using feature-distance\n",
      "Number of initial annotations: 1267.0\n",
      "Average F1 score for RF after initial user interaction:    0.3617\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ed133d8ec54ddc9f34906f35247e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User interaction:   0%|          | 0/20 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # Annotations | F1 Score\n",
      "----------|---------------|---------\n",
      "        1 |          2170 | 0.4283\n",
      "        2 |          2988 | 0.4441\n",
      "        3 |          3779 | 0.4545\n",
      "        4 |          4573 | 0.4605\n",
      "        5 |          5465 | 0.4643\n",
      "       10 |          9357 | 0.4847\n",
      "       15 |         12932 | 0.4953\n",
      "       20 |         16426 | 0.5125\n"
     ]
    }
   ],
   "source": [
    "ns_fd, ans_fd, f1s_fd, rf_preds_fd, uncs_pc_fd, uncs_fd = train(method='feature-distance', n_epochs=20, measures=['feature-distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b0a90",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f63d23b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using random\n",
      "Number of initial annotations: 1267.0\n",
      "Average F1 score for RF after initial user interaction:    0.3617\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5973ae4c818d46e6bb45cb5a19fda5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User interaction:   0%|          | 0/20 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # Annotations | F1 Score\n",
      "----------|---------------|---------\n",
      "        1 |          2243 | 0.4368\n",
      "        2 |          3300 | 0.4568\n",
      "        3 |          4265 | 0.4847\n",
      "        4 |          5280 | 0.4880\n",
      "        5 |          6337 | 0.5053\n",
      "       10 |         11494 | 0.5356\n",
      "       15 |         16644 | 0.5616\n",
      "       20 |         21655 | 0.5893\n"
     ]
    }
   ],
   "source": [
    "ns_r, ans_r, f1s_r, rf_preds_r, uncs_pc_r, uncs_r = train(method='random', n_epochs=20, measures=['random'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "244b5fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasse 1: (tensor([-1.,  0.,  1.], dtype=torch.float64), tensor([ 183390, 2861184,    4051]))\n",
      "Klasse 2: (tensor([-1.,  0.,  1.], dtype=torch.float64), tensor([    582, 3007134,   40909]))\n",
      "Klasse 3: (tensor([-1.,  0.,  1.], dtype=torch.float64), tensor([    307, 3009528,   38790]))\n",
      "Klasse 4: (tensor([0., 1.], dtype=torch.float64), tensor([3045455,    3170]))\n",
      "Klasse 5: (tensor([-1.,  0.,  1.], dtype=torch.float64), tensor([  15093, 2967059,   66473]))\n"
     ]
    }
   ],
   "source": [
    "x = rf_preds[-1] - dataset.label\n",
    "print(f'Klasse 1: {x[0].unique(return_counts=True)}')\n",
    "print(f'Klasse 2: {x[1].unique(return_counts=True)}')\n",
    "print(f'Klasse 3: {x[2].unique(return_counts=True)}')\n",
    "print(f'Klasse 4: {x[3].unique(return_counts=True)}')\n",
    "print(f'Klasse 5: {x[4].unique(return_counts=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "52664ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.,  0.], dtype=torch.float64), tensor([1063, 1567]))\n"
     ]
    }
   ],
   "source": [
    "mask = dataset.annotations.any(dim=0)\n",
    "y = dataset.annotations[:, mask] - dataset.label[:, mask]\n",
    "z = y.sum(dim=0)\n",
    "print(z.unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddadeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, annots in enumerate(ans_e):\n",
    "    if i in (0,1,2,3,4, 9, 14, 19):\n",
    "        torch.save(annots, f'annotations/annots_{i+1}_entropy.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43857ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, annots in enumerate(ans_sd):\n",
    "    if i in (0,1,2,3,4, 9, 14, 19):\n",
    "        torch.save(annots, f'annotations/annots_{i+1}_sd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d0845c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, annots in enumerate(ans_fd):\n",
    "    if i in (0,1,2,3,4, 9, 14, 19):\n",
    "        torch.save(annots, f'annotations/annots_{i+1}_fd.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df06a66",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc162e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_maps = [torch.ne(rf_prediction, dataset.label) * 1 for rf_prediction in rf_preds]\n",
    "error_maps_mean = [torch.any(error_map, dim=0) * 1 for error_map in error_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ffbe52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15243125, 3])\n"
     ]
    }
   ],
   "source": [
    "abc = uncs_pc[0]['entropy'].flatten()\n",
    "defg = uncs_pc[0]['spatial-distance'].flatten()\n",
    "hijk = uncs_pc[0]['feature-distance'].flatten()\n",
    "lmno = torch.stack((abc, defg, hijk), dim=1)\n",
    "print(lmno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d4c3a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f058dea6791482e84fba4642a53dcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculation:   0%|          | 0/20 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koeffizient für Iteration 1: Entropy: [[6.80838514]], SD: [[0.03420244]], FD: [[5.14872597]]\n",
      "Koeffizient für Iteration 1: Combined: [[ 6.72757369 -0.0340217   0.63251322]]\n",
      "Koeffizient für Iteration 2: Entropy: [[7.02794357]], SD: [[0.03219433]], FD: [[5.32265189]]\n",
      "Koeffizient für Iteration 2: Combined: [[ 6.73033755 -0.03509393  1.2549075 ]]\n",
      "Koeffizient für Iteration 3: Entropy: [[6.6982623]], SD: [[0.02620928]], FD: [[5.05152733]]\n",
      "Koeffizient für Iteration 3: Combined: [[ 6.29420073 -0.0529336   1.9025076 ]]\n",
      "Koeffizient für Iteration 4: Entropy: [[6.8116268]], SD: [[0.02158613]], FD: [[4.87395059]]\n",
      "Koeffizient für Iteration 4: Combined: [[ 6.59178342 -0.06249266  1.17257243]]\n",
      "Koeffizient für Iteration 5: Entropy: [[7.19238669]], SD: [[0.01998846]], FD: [[4.75844355]]\n",
      "Koeffizient für Iteration 5: Combined: [[ 7.08846542 -0.05720406  0.35012152]]\n",
      "Koeffizient für Iteration 6: Entropy: [[7.31138677]], SD: [[0.02119565]], FD: [[4.94982176]]\n",
      "Koeffizient für Iteration 6: Combined: [[ 7.19909338 -0.05252118  0.28792025]]\n",
      "Koeffizient für Iteration 7: Entropy: [[7.46501753]], SD: [[0.02088356]], FD: [[4.98891402]]\n",
      "Koeffizient für Iteration 7: Combined: [[ 7.32527822 -0.05210517  0.32009922]]\n",
      "Koeffizient für Iteration 8: Entropy: [[7.4728109]], SD: [[0.01655445]], FD: [[4.91746136]]\n",
      "Koeffizient für Iteration 8: Combined: [[ 7.25948023 -0.07076499  0.66780789]]\n",
      "Koeffizient für Iteration 9: Entropy: [[7.65695715]], SD: [[0.01807795]], FD: [[5.01641968]]\n",
      "Koeffizient für Iteration 9: Combined: [[ 7.44588136 -0.06067749  0.63144986]]\n",
      "Koeffizient für Iteration 10: Entropy: [[7.88620087]], SD: [[0.0149105]], FD: [[5.01443593]]\n",
      "Koeffizient für Iteration 10: Combined: [[ 7.63706198 -0.07136907  0.64764374]]\n",
      "Koeffizient für Iteration 11: Entropy: [[7.80623337]], SD: [[0.01361651]], FD: [[5.01710035]]\n",
      "Koeffizient für Iteration 11: Combined: [[ 7.55980865 -0.07544111  0.72586051]]\n",
      "Koeffizient für Iteration 12: Entropy: [[8.16946721]], SD: [[0.01329917]], FD: [[5.02519462]]\n",
      "Koeffizient für Iteration 12: Combined: [[ 7.86130355 -0.06921299  0.66288778]]\n",
      "Koeffizient für Iteration 13: Entropy: [[8.31745989]], SD: [[0.01391267]], FD: [[4.99480433]]\n",
      "Koeffizient für Iteration 13: Combined: [[ 8.1019051  -0.06081465  0.14810018]]\n",
      "Koeffizient für Iteration 14: Entropy: [[8.41426768]], SD: [[0.00948413]], FD: [[4.77357729]]\n",
      "Koeffizient für Iteration 14: Combined: [[ 8.16773366 -0.08207386  0.01620752]]\n",
      "Koeffizient für Iteration 15: Entropy: [[8.46227659]], SD: [[0.00888615]], FD: [[4.79644047]]\n",
      "Koeffizient für Iteration 15: Combined: [[ 8.1680069  -0.08400236  0.18326467]]\n",
      "Koeffizient für Iteration 16: Entropy: [[8.48307514]], SD: [[0.00942583]], FD: [[4.81521878]]\n",
      "Koeffizient für Iteration 16: Combined: [[ 8.22003659 -0.07857311  0.06067939]]\n",
      "Koeffizient für Iteration 17: Entropy: [[8.46032853]], SD: [[0.00933179]], FD: [[4.81248473]]\n",
      "Koeffizient für Iteration 17: Combined: [[ 8.17852393 -0.07791324  0.10570233]]\n",
      "Koeffizient für Iteration 18: Entropy: [[8.41374414]], SD: [[0.00800668]], FD: [[4.84997114]]\n",
      "Koeffizient für Iteration 18: Combined: [[ 8.11880232 -0.08526206  0.20948633]]\n",
      "Koeffizient für Iteration 19: Entropy: [[8.42593934]], SD: [[0.00739057]], FD: [[4.83043808]]\n",
      "Koeffizient für Iteration 19: Combined: [[ 8.12384395 -0.08665314  0.13084176]]\n",
      "Koeffizient für Iteration 20: Entropy: [[8.41835179]], SD: [[0.00618593]], FD: [[4.83350501]]\n",
      "Koeffizient für Iteration 20: Combined: [[ 8.07763439 -0.08778883  0.23979653]]\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the logistic regression model\n",
    "for i, (unc_map, error_map) in enumerate(tqdm(zip(uncs_pc, error_maps),total=len(uncs_pc), desc='Calculation', unit='iteration')):\n",
    "    unc_map_e = unc_map['entropy'].flatten()\n",
    "    unc_map_sd = unc_map['spatial-distance'].flatten()\n",
    "    unc_map_fd = unc_map['feature-distance'].flatten()\n",
    "    logreg_e = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "    logreg_e.fit(unc_map_e.reshape(-1, 1), error_map.flatten())\n",
    "    logreg_sd = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "    logreg_sd.fit(unc_map_sd.reshape(-1, 1), error_map.flatten())\n",
    "    logreg_fd = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "    logreg_fd.fit(unc_map_fd.reshape(-1, 1), error_map.flatten())\n",
    "    print(f\"Koeffizient für Iteration {i+1}: Entropy: {logreg_e.coef_}, SD: {logreg_sd.coef_}, FD: {logreg_fd.coef_}\")\n",
    "    logreg_c = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "    logreg_c.fit(torch.stack((unc_map_e, unc_map_sd, unc_map_fd), dim=1), error_map.flatten())\n",
    "    print(f\"Koeffizient für Iteration {i+1}: Combined: {logreg_c.coef_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
