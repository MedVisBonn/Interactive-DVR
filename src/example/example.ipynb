{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b8d26-e301-49fd-b2bb-a185310027f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os, sys\n",
    "#from time import time\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "#from torch.cuda.amp import GradScaler, autocast\n",
    "#from torch.utils.data._utils.collate import default_collate\n",
    "#import copy\n",
    "#from time import time\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import AEDataset\n",
    "from trainer import Trainer, WeakSupervisionTrainer\n",
    "from model import DualBranchAE\n",
    "from utils import *\n",
    "from losses import MSELoss\n",
    "from pretrainer import PreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211305c3-326d-4c47-bbec-5087737b3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lennartz/HiWi/Interactive-DVR/src/example\n"
     ]
    }
   ],
   "source": [
    "# %cd example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb22710-307a-4146-8bdd-57bba48aad1e",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We currently work on the HPC data and within this, we built two different segmentation tasks. Further details are in the paper https://cg.cs.uni-bonn.de/backend/v1/files/publications/torayev-vcbm2020.pdf. Neither the whole dataset nor the model are in this repo. We will set you up once you started your work and give your access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c199b3f-5e9b-4d46-acdb-3770417afc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of annotations: 9719.0\n",
      "dict_keys(['input', 'target', 'weight', 'mask'])\n"
     ]
    }
   ],
   "source": [
    "# which tasks are used is handled by \"set\". 1 is a binary task for debugging, 2 is multi-class \n",
    "# and so is 3 but with asymmetric classes w.r.t. the saggital plane (harder). Details for \n",
    "# set 2 and 3 are in the paper.\n",
    "# 'modality' handles the target provided by __getitem__. Options are reconstruction and segmentation.\n",
    "# When segmentation is selected, the labels are taken from the annotations attribute. This is also where\n",
    "# the user-model interacts. Ground truth masks are in the label attribute. All other parameters are\n",
    "# from past experiments and alter the behaviour. This project is a couple years old.\n",
    "\n",
    "# normalize is usually set to true. Simply normalizes the input. Augment is legacy, we didn't have much success\n",
    "# with data augmentation. balance takes care of data balancing during a batch. Some classes are under-\n",
    "# represented so we show them to the model more often. It helps quite a bit during training so consider \n",
    "# integrating it. We can talk about how this is done in detail. init defines how the user-model behaves. We considered\n",
    "# different behaviours w.r.t. to annotation style and quantity and such. To_gpu moves ALL data to GPU. Since we\n",
    "# only work on a single volume (i.e. couple hundred slices) we move everything to GPU and avoid latency in \n",
    "# dataloading. Takes a hefty chunk out of the VRAM though but makes things faster.\n",
    "\n",
    "# Feel free to re-write anything you want. This is partly dated code that could use a re-write anyways.\n",
    "\n",
    "# Example:\n",
    "# make a config first. This handles globals and is used through-out the script. Many things that were tried in\n",
    "# experiments later have not yet made it into the config, but most have.\n",
    "\n",
    "cfg = {\n",
    "    # CONFIG\n",
    "    'name': 'location-unsupervised',\n",
    "    'project': 'IDVR-localization_pretrain',\n",
    "    'log': False,\n",
    "    'rank': 0,\n",
    "    \n",
    "    # DATA\n",
    "    'data_dir': '../../../data/784565/Diffusion/',\n",
    "    'data_path': '../../../data/784565/Diffusion/data.nii',\n",
    "    'active_mask_path': '../../../data/784565/Diffusion/nodif_brain_mask.nii.gz',\n",
    "    \n",
    "    # SELF SUPERVISED PRE-TRAINING\n",
    "    's_n_epochs': 20,\n",
    "    's_batch_size': 16, # default: 8\n",
    "    's_lr': 5e-4, #1e-4, 1e-5        \n",
    "    \n",
    "    # TRAINING WITH WEAK SUPERVISION\n",
    "    'p_n_epochs': 100,\n",
    "    'w_n_epochs': 10,\n",
    "    'w_batch_size': 2,\n",
    "    'w_lr': 5e-4,    #5e-5 \n",
    "    'w_eval_freq': 100,\n",
    "    \n",
    "    # RANDOM FOREST\n",
    "    'min_samples_leaf': 8,\n",
    "    \n",
    "    # USER MODEL\n",
    "    'init_voxels': 200,\n",
    "    'refinement_voxels': 200,\n",
    "    'num_interactions': 10,\n",
    "}\n",
    "\n",
    "# we set balance to true. This also effects the dataloader later\n",
    "balance=True\n",
    "dataset = AEDataset(cfg, modality='segmentation', normalize=True,\n",
    "                    set=2, augment=False, balance=balance, init='per_class', to_gpu=False)\n",
    "\n",
    "# currently, there are no annotations. We can also enforce this with clear_annotations()\n",
    "dataset.clear_annotation()\n",
    "# get initial annotations\n",
    "annot = dataset.initial_annotation(seed=42)\n",
    "# and update the dataset\n",
    "dataset.update_annotation(annot)\n",
    "print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "\n",
    "# The dataset currently always provides 4 items. Input (the image), target (the input for reconstruction or \n",
    "#  the annotations for segmentation), weights that mask out voxels which are not annotated for segmentation \n",
    "# and a brain mask for masking background during reconstruction\n",
    "item = dataset[0]\n",
    "print(item.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f901cd0-8b3b-4cb5-8b32-f89ab55a8595",
   "metadata": {},
   "source": [
    "# Model and Inference\n",
    "\n",
    "The overall pipeline is illustrated in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9e0d19-66c2-46ff-8f8c-bab0943aadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first, we do not have annotations but still need features for the Random Forest. So we pre-train \n",
    "# on a reconstruction task and later re-use the same Encoder (the part of the network that outputs our features)\n",
    "# and simply replace the decoder and resume training. \n",
    "\n",
    "# init the model with segmentation decoder. Have a look at the source code for additional guidance. The dataset\n",
    "# updates the config to contain labels. We initialize with one channel per class.\n",
    "model = DualBranchAE(encoder    = 'dual',\n",
    "                     decoder    = 'segmentation',\n",
    "                     in_size    = 145,\n",
    "                     n_classes  = len(cfg['labels']),\n",
    "                     thresholds = 'learned') #.to(cfg['rank'])\n",
    "\n",
    "# example model from one of the experiments\n",
    "model_path = 'example_dual_xy_0_best.pt'\n",
    "\n",
    "# load the components\n",
    "checkpoint           = torch.load(model_path)\n",
    "model_state_dict     = checkpoint['model_dict']\n",
    "encoder_state_dict   = {k.replace('encoder.', ''): v for k, v in model_state_dict.items() if 'encoder' in k}\n",
    "\n",
    "# copy encoder weights to model. Decoder weights remain as they are\n",
    "model.encoder.load_state_dict(encoder_state_dict, strict=True)\n",
    "\n",
    "# Define the dataloader. If we use balanced sampling in the dataset, we also need the custom balanced_collate \n",
    "# function in the dataloader. This handles the unusal batching logic.\n",
    "\n",
    "if balance:\n",
    "    loader  = DataLoader(dataset, \n",
    "                         batch_size=cfg['w_batch_size'], \n",
    "                         shuffle=True, \n",
    "                         drop_last=False, \n",
    "                         collate_fn=balanced_collate)\n",
    "else:\n",
    "    loader  = DataLoader(dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True, \n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c31b2b-2858-4b44-b99a-01a1caa794db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for RF after initial user interaction:    0.3109\n",
      "Average F1 score for RF after additional user interaction: 0.3959\n"
     ]
    }
   ],
   "source": [
    "# For evaluation, we are interested in the Random Forest prediction based on\n",
    "# the CNN features. \n",
    "\n",
    "# write checkpoints for stuff that changes the behaviour of the dataset.\n",
    "# E.g. balancing changes the __getitem__ method and thus influences \n",
    "# evaluation. Turn it off and on later if needed.\n",
    "augment_checkpoint = dataset.augment\n",
    "balance_checkpoint = dataset.balance\n",
    "dataset.augment = False\n",
    "dataset.balance = False\n",
    "\n",
    "# define the layer you want the features from. This is usually the encoder output.\n",
    "f_layer = 'encoder'\n",
    "# Init the feature extractor. Have a look at PyTorchs Hook functionality.\n",
    "extractor = FeatureExtractor(model, layers=[f_layer])\n",
    "# Cache all features for a dataset and reformat/move to np for random forest\n",
    "hooked_results  = extractor(dataset)\n",
    "features = hooked_results[f_layer]\n",
    "features  = features.permute(0,2,3,1).numpy()\n",
    "# In the utils file are a bunch of evaluation scripts, some are not used anymore.\n",
    "# This one provides F1 scores for the whole dataset based on all ground truth labels\n",
    "# and also the predictions themselve. We need them later to update the annotations with\n",
    "# the user model.\n",
    "scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "# Turn dataset attributes to normal again\n",
    "dataset.augment = augment_checkpoint\n",
    "dataset.balance = balance_checkpoint\n",
    "\n",
    "# we can use the predictions to update the annotations via the usermodel\n",
    "annot = dataset.refinement_annotation(prediction=rf_prediction, seed=42)\n",
    "dataset.update_annotation(annot)\n",
    "scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "# Now you can change the model to your liking and try again. The scores from the RF\n",
    "# are the signal you need for evaluation, the rest is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62453306-a209-4f52-9bfb-7ab80c926537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of annotations: 16295.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f336b3-0977-4f2e-9812-043f9b37df2e",
   "metadata": {},
   "source": [
    "# Pretraining without labels\n",
    "\n",
    "Before we have any annotations, we pretrain with a reconstruction task that also tries to infer the location of voxels as additional signal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3351cb62-d9fe-4cee-afba-2c335fcfd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of outgoing channales depends on whether we also want to localize voxels in \n",
    "# the XY plane. If yes, its 290 and 288 o/w.\n",
    "localize = True\n",
    "out_channel = 290 if localize else 288\n",
    "model = DualBranchAE(encoder='dual',\n",
    "                     decoder='reconstruction',\n",
    "                     in_size=145,\n",
    "                     recon_channel=out_channel)\n",
    "    \n",
    "dataset = AEDataset(cfg, modality='reconstruction', normalize=True,\n",
    "                    set=2, augment=False, localize=localize, to_gpu=False)\n",
    "\n",
    "\n",
    "criterion = MSELoss()\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=False)\n",
    "description = 'Test' \n",
    "log = False\n",
    "\n",
    "pre_trainer = PreTrainer(model, criterion, loader, cfg, n_epochs=cfg['s_n_epochs'], \n",
    "                         lr=cfg['s_lr'], log=log, description=description,\n",
    "                         patience=8, es_mode='none', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac32673-dc92-40ea-84d0-78581c0dd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ed739-ae46-407b-9be5-0fe3383de2b0",
   "metadata": {},
   "source": [
    "# Weakly Supervised Training with Segmentation Head\n",
    "\n",
    "Once you have implemented your pipeline we can compare to what we currently do. Since this changes alot currently, we will go over it once you are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53520ca2-3c08-4596-9624-9df53ad871e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
