{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5b8d26-e301-49fd-b2bb-a185310027f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os, sys\n",
    "#from time import time\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "#from torch.cuda.amp import GradScaler, autocast\n",
    "#from torch.utils.data._utils.collate import default_collate\n",
    "#import copy\n",
    "#from time import time\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import AEDataset\n",
    "from trainer import Trainer, WeakSupervisionTrainer\n",
    "from model import DualBranchAE\n",
    "from utils import *\n",
    "from losses import MSELoss\n",
    "from pretrainer import PreTrainer\n",
    "from torchmetrics.classification import BinaryROC\n",
    "from scipy import ndimage\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc02d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload changes in .py files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "211305c3-326d-4c47-bbec-5087737b3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb22710-307a-4146-8bdd-57bba48aad1e",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We currently work on the HPC data and within this, we built two different segmentation tasks. Further details are in the paper https://cg.cs.uni-bonn.de/backend/v1/files/publications/torayev-vcbm2020.pdf. Neither the whole dataset nor the model are in this repo. We will set you up once you started your work and give your access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c199b3f-5e9b-4d46-acdb-3770417afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tasks are used is handled by \"set\". 1 is a binary task for debugging, 2 is multi-class \n",
    "# and so is 3 but with asymmetric classes w.r.t. the saggital plane (harder). Details for \n",
    "# set 2 and 3 are in the paper.\n",
    "# 'modality' handles the target provided by __getitem__. Options are reconstruction and segmentation.\n",
    "# When segmentation is selected, the labels are taken from the annotations attribute. This is also where\n",
    "# the user-model interacts with the dataset. Ground truth masks are in the label attribute. All other parameters are\n",
    "# from past experiments and alter the behaviour. This project has been around for a while, so some are not used anymore.\n",
    "\n",
    "# normalize is usually set to true. Simply normalizes the input. Augment is legacy, we didn't have much success\n",
    "# with data augmentation. balance takes care of data balancing during a batch. Some classes are under-\n",
    "# represented so we show them to the model more often. It helps quite a bit during training so consider \n",
    "# integrating it. We can talk about how this is done in detail once you start. init defines how the user-model behaves. \n",
    "# We considered different behaviours w.r.t. to annotation style and quantity and such. \n",
    "# To_gpu moves ALL data to GPU. Since we only work on a single volume (i.e. couple hundred slices) \n",
    "# we move everything to GPU and avoid latency in dataloading. Takes a hefty chunk out of the VRAM though \n",
    "# but makes things faster.\n",
    "\n",
    "# Feel free to re-write anything you want. This is partly dated code that could use a re-write anyways.\n",
    "\n",
    "# Example:\n",
    "# make a config first. This handles globals and is used through-out the script. Many things that were tried in\n",
    "# experiments later have not yet made it into the config, but most have.\n",
    "\n",
    "cfg = {\n",
    "    # CONFIG\n",
    "    'name': 'location-unsupervised',\n",
    "    'project': 'IDVR-localization_pretrain',\n",
    "    'log': False,\n",
    "    'rank': 0,\n",
    "    \n",
    "    # DATA\n",
    "    'data_dir': '../../../data/784565/Diffusion/',\n",
    "    'data_path': '../../../data/784565/Diffusion/data.nii',\n",
    "    'active_mask_path': '../../../data/784565/Diffusion/nodif_brain_mask.nii.gz',\n",
    "    \n",
    "    # SELF SUPERVISED PRE-TRAINING\n",
    "    's_n_epochs': 20,\n",
    "    's_batch_size': 16, # default: 8\n",
    "    's_lr': 5e-4, #1e-4, 1e-5        \n",
    "    \n",
    "    # TRAINING WITH WEAK SUPERVISION\n",
    "    'p_n_epochs': 100,\n",
    "    'w_n_epochs': 10,\n",
    "    'w_batch_size': 2,\n",
    "    'w_lr': 5e-4,    #5e-5 \n",
    "    'w_eval_freq': 100,\n",
    "    \n",
    "    # RANDOM FOREST\n",
    "    'min_samples_leaf': 8,\n",
    "    \n",
    "    # USER MODEL\n",
    "    'init_voxels': 200,\n",
    "    'refinement_voxels': 200,\n",
    "    'num_interactions': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66666ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of annotations: 9510.0\n",
      "dict_keys(['input', 'target', 'weight', 'mask'])\n"
     ]
    }
   ],
   "source": [
    "# we set balance to true. This also effects the dataloader later\n",
    "balance = False\n",
    "dataset = AEDataset(cfg, modality='segmentation', normalize=True,\n",
    "                    set=2, augment=False, balance=balance, init='per_class', to_gpu=False)\n",
    "\n",
    "# currently, there are no annotations. We can also enforce this with clear_annotations() at any point\n",
    "dataset.clear_annotation()\n",
    "# get initial annotations\n",
    "annot = dataset.initial_annotation(seed=42)\n",
    "# and update the dataset\n",
    "dataset.update_annotation(annot)\n",
    "print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "\n",
    "# The dataset currently always provides 4 items. Input (the image), target (the input for reconstruction or \n",
    "# the annotations for segmentation), weights that mask out voxels which are not annotated for segmentation \n",
    "# and a brain mask for masking background during reconstruction\n",
    "item = dataset[0]\n",
    "print(item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7373b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9fd91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288, 145, 145])\n",
      "torch.Size([5, 145, 145])\n",
      "torch.Size([1, 145, 145])\n",
      "torch.Size([145, 145])\n"
     ]
    }
   ],
   "source": [
    "print(item['input'].shape)\n",
    "print(item['target'].shape)\n",
    "print(item['weight'].shape)\n",
    "print(item['mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f901cd0-8b3b-4cb5-8b32-f89ab55a8595",
   "metadata": {},
   "source": [
    "# Model and Inference\n",
    "\n",
    "The overall pipeline is illustrated in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f9e0d19-66c2-46ff-8f8c-bab0943aadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first, we do not have annotations but still need features for the Random Forest. So we pre-train \n",
    "# on a reconstruction task and later re-use the same Encoder (the part of the network that outputs our features),\n",
    "# simply replace the decoder and resume training. \n",
    "\n",
    "# init the model with segmentation decoder. Have a look at the source code for additional guidance. The dataset\n",
    "# updates the config to contain labels. We initialize with one channel per class.\n",
    "model = DualBranchAE(encoder    = 'dual',\n",
    "                     decoder    = 'segmentation',\n",
    "                     in_size    = 145,\n",
    "                     n_classes  = len(cfg['labels']),\n",
    "                     thresholds = 'learned') #.to(cfg['rank'])\n",
    "\n",
    "# example model from one of the experiments\n",
    "#model_path = 'example_dual_xy_0_best.pt'\n",
    "model_path = 'models/Test_best.pt'\n",
    "\n",
    "# load the components\n",
    "checkpoint           = torch.load(model_path)\n",
    "model_state_dict     = checkpoint['model_dict']\n",
    "encoder_state_dict   = {k.replace('encoder.', ''): v for k, v in model_state_dict.items() if 'encoder' in k}\n",
    "\n",
    "# copy encoder weights to model. Decoder weights remain as they are, initialized as random\n",
    "model.encoder.load_state_dict(encoder_state_dict, strict=True)\n",
    "\n",
    "# Define the dataloader. If we use balanced sampling in the dataset, we also need the custom balanced_collate \n",
    "# function in the dataloader. This handles the unusal batching logic.\n",
    "\n",
    "if balance:\n",
    "    loader  = DataLoader(dataset, \n",
    "                         batch_size=cfg['w_batch_size'], \n",
    "                         shuffle=True, \n",
    "                         drop_last=False, \n",
    "                         collate_fn=balanced_collate)\n",
    "else:\n",
    "    loader  = DataLoader(dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True, \n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c31b2b-2858-4b44-b99a-01a1caa794db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, we are interested in the Random Forest (RF) prediction based on\n",
    "# the CNN features. \n",
    "\n",
    "# write checkpoints for stuff that changes the behaviour of the dataset.\n",
    "# E.g. balancing changes the __getitem__ method and thus influences \n",
    "# evaluation. Turn it off and on later if needed.\n",
    "augment_checkpoint = dataset.augment\n",
    "balance_checkpoint = dataset.balance\n",
    "dataset.augment = False\n",
    "dataset.balance = False\n",
    "\n",
    "# define the layer you want the features from. This is usually the encoder output.\n",
    "f_layer = 'encoder'\n",
    "# Init the feature extractor. Have a look at PyTorchs Hook functionality.\n",
    "extractor = FeatureExtractor(model, layers=[f_layer])\n",
    "# Cache all features for a dataset and reformat/move to numpy for random forest stuff\n",
    "hooked_results  = extractor(dataset)\n",
    "features = hooked_results[f_layer]\n",
    "features  = features.permute(0,2,3,1).numpy()\n",
    "# In the utils file are a bunch of evaluation scripts, some are not used anymore.\n",
    "# This one provides F1 scores for the whole dataset based on all ground truth labels\n",
    "# and also the predictions themselve as given by the RF. We need them later to update the annotations with\n",
    "# the user model.\n",
    "\n",
    "# Turn dataset attributes to normal again\n",
    "dataset.augment = augment_checkpoint\n",
    "dataset.balance = balance_checkpoint\n",
    "\n",
    "\n",
    " #Now you can change the model and features to your liking and try again (e.g. via constrastive learning ;)).\n",
    " #The scores from the RF are the signal you need for evaluation, the rest is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078f91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of annotations: 9510.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using entropy:\n",
      "Average F1 score for RF after initial user interaction:    0.3152\n",
      "number of annotations: 15463.0\n",
      "Average F1 score for RF after additional user interaction: 0.3745\n",
      "number of annotations: 20730.0\n",
      "Average F1 score for RF after additional user interaction: 0.3876\n",
      "number of annotations: 25456.0\n",
      "Average F1 score for RF after additional user interaction: 0.4184\n",
      "number of annotations: 29263.0\n",
      "Average F1 score for RF after additional user interaction: 0.4243\n",
      "number of annotations: 34116.0\n",
      "Average F1 score for RF after additional user interaction: 0.4290\n"
     ]
    }
   ],
   "source": [
    "print('Selection using entropy:')\n",
    "scores, rf_prediction, entropy_map, x, y, z, v, w = evaluate_RF_with_uncertainty(dataset, features, cfg)\n",
    "print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "for i in range(5):\n",
    "    annot = dataset.uncertainty_refinement_annotation(prediction=rf_prediction, uncertainty_map=entropy_map, seed=42)\n",
    "    dataset.update_annotation(annot)\n",
    "    print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    scores, rf_prediction, entropy_map, x, y, z, v, w = evaluate_RF_with_uncertainty(dataset, features, cfg)\n",
    "    print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using spatial distance:\n",
      "Average F1 score for RF after initial user interaction:    0.3152\n",
      "number of annotations: 13862.0\n",
      "Average F1 score for RF after additional user interaction: 0.3823\n",
      "number of annotations: 17573.0\n",
      "Average F1 score for RF after additional user interaction: 0.3905\n",
      "number of annotations: 20408.0\n",
      "Average F1 score for RF after additional user interaction: 0.4115\n",
      "number of annotations: 23559.0\n",
      "Average F1 score for RF after additional user interaction: 0.4310\n",
      "number of annotations: 26581.0\n",
      "Average F1 score for RF after additional user interaction: 0.4353\n"
     ]
    }
   ],
   "source": [
    "print('Selection using spatial distance:')\n",
    "scores, rf_prediction, entropy_map, em_m, sd, sd_pc, fd, fd_pc = evaluate_RF_with_uncertainty(dataset, features, cfg)\n",
    "print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "for i in range(5):\n",
    "    annot = dataset.uncertainty_refinement_annotation(prediction=rf_prediction, uncertainty_map=sd_pc, seed=42)\n",
    "    dataset.update_annotation(annot)\n",
    "    print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    scores, rf_prediction, entropy_map, em_m, sd, sd_pc, fd, fd_pc = evaluate_RF_with_uncertainty(dataset, features, cfg)\n",
    "    print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using feature distance:\n",
      "Average F1 score for RF after initial user interaction:    0.3152\n",
      "number of annotations: 14036.0\n",
      "Average F1 score for RF after additional user interaction: 0.3720\n",
      "number of annotations: 17377.0\n",
      "Average F1 score for RF after additional user interaction: 0.3974\n",
      "number of annotations: 20672.0\n",
      "Average F1 score for RF after additional user interaction: 0.4059\n",
      "number of annotations: 23926.0\n",
      "Average F1 score for RF after additional user interaction: 0.4125\n",
      "number of annotations: 26672.0\n",
      "Average F1 score for RF after additional user interaction: 0.4179\n"
     ]
    }
   ],
   "source": [
    "print('Selection using feature distance:')\n",
    "scores, rf_prediction, entropy_map, em_m, sd, sd_pc, fd, fd_pc = evaluate_RF_with_uncertainty(dataset, features, cfg)\n",
    "print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "for i in range(5):\n",
    "    annot = dataset.uncertainty_refinement_annotation(prediction=rf_prediction, uncertainty_map=fd_pc, seed=42)\n",
    "    dataset.update_annotation(annot)\n",
    "    print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    scores, rf_prediction, entropy_map, em_m, sd, sd_pc, fd, fd_pc = evaluate_RF_with_uncertainty(dataset, features, cfg)\n",
    "    print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection using ground truth:\n",
      "Average F1 score for RF after initial user interaction:    0.3152\n",
      "number of annotations: 16194.0\n",
      "Average F1 score for RF after additional user interaction: 0.3604\n",
      "number of annotations: 22890.0\n",
      "Average F1 score for RF after additional user interaction: 0.4065\n",
      "number of annotations: 28969.0\n",
      "Average F1 score for RF after additional user interaction: 0.4237\n",
      "number of annotations: 35380.0\n",
      "Average F1 score for RF after additional user interaction: 0.4551\n",
      "number of annotations: 39962.0\n",
      "Average F1 score for RF after additional user interaction: 0.4915\n"
     ]
    }
   ],
   "source": [
    "print('Selection using ground truth:')\n",
    "scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "print(f\"Average F1 score for RF after initial user interaction:    {scores['Avg_f1_tracts'].item():.4f}\")\n",
    "for i in range(5):\n",
    "    annot = dataset.refinement_annotation(prediction=rf_prediction, seed=42)\n",
    "    dataset.update_annotation(annot)\n",
    "    print(f\"number of annotations: {dataset.annotations.detach().cpu().sum()}\")\n",
    "    scores, rf_prediction = evaluate_RF(dataset, features, cfg)\n",
    "    print(f\"Average F1 score for RF after additional user interaction: {scores['Avg_f1_tracts'].item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
